#!/usr/bin/env python3
"""
æµ‹è¯• BM25 æœç´¢å¼•æ“
"""
import json
from pathlib import Path
from search_engine import PaperSearchEngine, search_papers_bm25

def test_search_engine():
    """æµ‹è¯•æœç´¢å¼•æ“åŸºæœ¬åŠŸèƒ½"""
    
    print("=" * 60)
    print("æµ‹è¯• BM25 æœç´¢å¼•æ“")
    print("=" * 60)
    
    # 1. åŠ è½½æµ‹è¯•æ•°æ®
    print("\n1. åŠ è½½è®ºæ–‡æ•°æ®...")
    data_dir = Path("papers_data")
    json_files = list(data_dir.glob("papers_*.json"))
    
    if not json_files:
        print("âŒ æœªæ‰¾åˆ°è®ºæ–‡æ•°æ®æ–‡ä»¶ï¼")
        print(f"   è¯·ç¡®ä¿ {data_dir} ç›®å½•ä¸‹æœ‰ papers_*.json æ–‡ä»¶")
        return False
    
    # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
    json_file = sorted(json_files)[-1]
    print(f"   ä½¿ç”¨æ–‡ä»¶: {json_file}")
    
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # æå–è®ºæ–‡åˆ—è¡¨
    if isinstance(data, list):
        papers = data
    elif isinstance(data, dict) and 'papers' in data:
        papers = data['papers']
    else:
        print("âŒ æ— æ³•è§£æè®ºæ–‡æ•°æ®æ ¼å¼ï¼")
        return False
    
    print(f"   åŠ è½½äº† {len(papers)} ç¯‡è®ºæ–‡")
    
    # 2. åˆ›å»ºæœç´¢å¼•æ“
    print("\n2. åˆå§‹åŒ–æœç´¢å¼•æ“...")
    engine = PaperSearchEngine(index_path="./test_search_index")
    
    # 3. æ„å»ºç´¢å¼•
    print("\n3. æ„å»ºæœç´¢ç´¢å¼•...")
    engine.build_index_from_papers(papers)
    
    # 4. æŸ¥çœ‹ç´¢å¼•çŠ¶æ€
    stats = engine.get_index_stats()
    print(f"   ç´¢å¼•çŠ¶æ€: {stats}")
    
    if stats['num_documents'] == 0:
        print("âŒ ç´¢å¼•æ„å»ºå¤±è´¥ï¼")
        return False
    
    print(f"   âœ… ç´¢å¼•æ„å»ºæˆåŠŸï¼Œå…± {stats['num_documents']} ç¯‡è®ºæ–‡")
    
    # 5. æµ‹è¯•æœç´¢
    print("\n4. æµ‹è¯•æœç´¢åŠŸèƒ½...")
    
    test_queries = [
        "transformer",
        "attention mechanism",
        "large language model",
        "neural network"
    ]
    
    for query in test_queries:
        print(f"\n   æŸ¥è¯¢: '{query}'")
        results = engine.search(query, max_results=5)
        
        if results:
            print(f"   âœ… æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            for i, result in enumerate(results[:3], 1):
                print(f"      {i}. {result['title'][:80]}...")
                print(f"         Score: {result['search_score']:.4f}")
        else:
            print(f"   âš ï¸ æœªæ‰¾åˆ°ç»“æœ")
    
    # 6. æµ‹è¯•ä¾¿æ·å‡½æ•°
    print("\n5. æµ‹è¯•ä¾¿æ·å‡½æ•°...")
    results = search_papers_bm25(
        query="transformer",
        papers=papers,
        search_engine=engine
    )
    print(f"   âœ… search_papers_bm25 è¿”å› {len(results)} ä¸ªç»“æœ")
    
    # 7. æ¸…ç†
    print("\n6. æ¸…ç†æµ‹è¯•ç´¢å¼•...")
    engine.clear_index()
    print("   âœ… æµ‹è¯•ç´¢å¼•å·²æ¸…ç†")
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)
    
    return True


def test_simple_usage():
    """æµ‹è¯•æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼"""
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç®€å•ä½¿ç”¨æ–¹å¼")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿè®ºæ–‡æ•°æ®
    papers = [
        {
            "id": "2301.00001",
            "arxiv_id": "2301.00001",
            "title": "Attention Is All You Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks.",
            "authors": ["Ashish Vaswani", "Noam Shazeer"],
            "categories": ["cs.AI", "cs.LG"],
            "published_date": "2023-01-01"
        },
        {
            "id": "2301.00002",
            "arxiv_id": "2301.00002",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers",
            "abstract": "We introduce a new language representation model called BERT.",
            "authors": ["Jacob Devlin", "Ming-Wei Chang"],
            "categories": ["cs.CL", "cs.AI"],
            "published_date": "2023-01-02"
        },
        {
            "id": "2301.00003",
            "arxiv_id": "2301.00003",
            "title": "GPT-4 Technical Report",
            "abstract": "We report the development of GPT-4, a large-scale multimodal model.",
            "authors": ["OpenAI"],
            "categories": ["cs.AI"],
            "published_date": "2023-01-03"
        }
    ]
    
    print(f"\nä½¿ç”¨ {len(papers)} ç¯‡æµ‹è¯•è®ºæ–‡")
    
    # ä½¿ç”¨ä¾¿æ·å‡½æ•°
    results = search_papers_bm25(
        query="transformer",
        papers=papers
    )
    
    print(f"\næœç´¢ 'transformer' çš„ç»“æœ:")
    for i, paper in enumerate(results, 1):
        print(f"{i}. {paper['title']}")
        if 'search_score' in paper:
            print(f"   Score: {paper['search_score']:.4f}")
    
    print("\nâœ… ç®€å•ä½¿ç”¨æµ‹è¯•é€šè¿‡ï¼")
    
    # æ¸…ç†
    from pathlib import Path
    import shutil
    index_path = Path("./search_index")
    if index_path.exists():
        shutil.rmtree(index_path)


if __name__ == "__main__":
    try:
        # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
        test_search_engine()
        
        # æµ‹è¯•ç®€å•ä½¿ç”¨
        test_simple_usage()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•æˆåŠŸï¼æœç´¢å¼•æ“å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
        
    except ImportError as e:
        print("\nâŒ å¯¼å…¥é”™è¯¯ï¼")
        print(f"   {e}")
        print("\nè¯·å®‰è£… tantivy:")
        print("   pip install tantivy")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
