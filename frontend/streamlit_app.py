"""
TEST PAGE
"""

import streamlit as st
from datetime import datetime, timedelta
import pandas as pd
from typing import List, Dict, Optional
import json
import os
from pathlib import Path
import io

import sys
sys.path.append("/home/hhy/project/paper-agent/papers.cool-main/backend/utils")
# å¯¼å…¥ BM25 æœç´¢å¼•æ“
try:
    from search_engine import PaperSearchEngine, search_papers_bm25
    SEARCH_ENGINE_AVAILABLE = True
except ImportError:
    import traceback
    traceback.print_exc()
    SEARCH_ENGINE_AVAILABLE = False
    st.warning("âš ï¸ Tantivy æœç´¢å¼•æ“ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€å•æœç´¢æ¨¡å¼ã€‚è¯·å®‰è£…: pip install tantivy")

import re


# é¡µé¢é…ç½®
st.set_page_config(
    page_title="TEST PAGE",
    page_icon="ğŸ“",
)

# æ•°æ®ç›®å½• - å‡è®¾è®ºæ–‡æ•°æ®æŒ‰æ—¥æœŸå­˜å‚¨ä¸ºJSONæ–‡ä»¶
DATA_DIR = os.getenv("DATA_DIR", "/home/hhy/project/paper-agent/papers-agent/papers_data")
#DATA_DIR = os.getenv("DATA_DIR", "./papers_data")

# åç«¯ API åœ°å€
API_BASE_URL = st.secrets.get("API_BASE_URL", "http://localhost:8000")

# åˆå§‹åŒ– session state
if "starred_papers" not in st.session_state:
    st.session_state.starred_papers = set()
if "viewed_papers" not in st.session_state:
    st.session_state.viewed_papers = set()
if "selected_categories" not in st.session_state:
    st.session_state.selected_categories = ["cs.AI", "cs.CL", "cs.LG"]

if "search_engine" not in st.session_state and SEARCH_ENGINE_AVAILABLE:
    st.session_state.search_engine = None

if "search_mode" not in st.session_state:
    st.session_state.search_mode = "bm25" if SEARCH_ENGINE_AVAILABLE else "simple"


# ArXiv åˆ†ç±»å®šä¹‰
ARXIV_CATEGORIES = {
    "Artificial Intelligence (cs.AI)": "cs.AI",
    "Computation and Language (cs.CL)": "cs.CL",
    "Computer Vision (cs.CV)": "cs.CV",
    "Machine Learning (cs.LG)": "cs.LG",
    "Neural and Evolutionary Computing (cs.NE)": "cs.NE",
    "Computational Complexity (cs.CC)": "cs.CC",
    "Statistics - Machine Learning (stat.ML)": "stat.ML",
    "PubMed (Medical Research)": "PubMed",
}

# Pills èƒ¶å›Šå¼é¢œè‰²å®šä¹‰ - ä½¿ç”¨æŸ”å’Œçš„é…è‰²æ–¹æ¡ˆ
CATEGORY_COLORS = {
    "cs.AI": {"bg": "#FFE5E5", "border": "#FF6B6B", "text": "#CC0000"},           # æŸ”å’Œçº¢
    "cs.CL": {"bg": "#E0F7F7", "border": "#4ECDC4", "text": "#008B8B"},           # æŸ”å’Œé’
    "cs.CV": {"bg": "#E3F2FD", "border": "#45B7D1", "text": "#1565C0"},           # æŸ”å’Œè“
    "cs.LG": {"bg": "#E8F5E9", "border": "#96CEB4", "text": "#2E7D32"},           # æŸ”å’Œç»¿
    "cs.NE": {"bg": "#FFF9E6", "border": "#FFEAA7", "text": "#F57F17"},           # æŸ”å’Œé»„
    "cs.CC": {"bg": "#F5F5F5", "border": "#DFE6E9", "text": "#616161"},           # æŸ”å’Œç°
    "stat.ML": {"bg": "#F3E5F5", "border": "#A29BFE", "text": "#6A1B9A"},         # æŸ”å’Œç´«
    "PubMed": {"bg": "#FFEBEE", "border": "#EF5350", "text": "#C62828"},         # åŒ»ç–—çº¢
}


def load_papers_from_json(date_str: str, selected_categories: List[str] = None) -> List[Dict]:
    """
    ä»JSONæ–‡ä»¶åŠ è½½æŒ‡å®šæ—¥æœŸçš„è®ºæ–‡æ•°æ®
    æ”¯æŒæ–°çš„æŒ‰ç±»åˆ«ç»„ç»‡æ ¼å¼å’Œæ—§çš„æ€»æ–‡ä»¶æ ¼å¼:
    1. æ–°æ ¼å¼: papers_data/cs.AI/papers_YYYY-MM-DD_100percent.json (æŒ‰ç±»åˆ«æ–‡ä»¶å¤¹)
    2. æ—§æ ¼å¼: papers_data/papers_YYYY-MM-DD_100percent.json (æ€»æ–‡ä»¶)
    """
    data_path = Path(DATA_DIR)
    all_papers = []

    # é¦–å…ˆå°è¯•æ–°çš„æŒ‰ç±»åˆ«ç»„ç»‡æ ¼å¼
    category_files_found = False

    # ç¡®å®šè¦åŠ è½½çš„ç±»åˆ«
    categories_to_load = selected_categories if selected_categories else ARXIV_CATEGORIES.values()

    for category in categories_to_load:
        category_dir = data_path / category
        json_file = category_dir / f"papers_{date_str}_100percent.json"

        if json_file.exists():
            category_files_found = True
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                    # å¤„ç†æ•°æ®æ ¼å¼
                    if isinstance(data, dict) and "papers" in data:
                        papers = data["papers"]
                        all_papers.extend(papers)
                    else:
                        continue

            except Exception as e:
                continue

    # å¦‚æœæ‰¾åˆ°äº†ç±»åˆ«æ–‡ä»¶ï¼Œç›´æ¥è¿”å›åˆå¹¶çš„ç»“æœ
    if category_files_found and all_papers:
        # å»é‡ï¼ˆæŒ‰ arxiv_idï¼‰
        unique_papers = {}
        for paper in all_papers:
            arxiv_id = paper.get("arxiv_id", paper.get("id", ""))
            if arxiv_id and arxiv_id not in unique_papers:
                unique_papers[arxiv_id] = paper

        result_papers = list(unique_papers.values())
        return result_papers

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç±»åˆ«æ–‡ä»¶ï¼Œå°è¯•æ—§çš„æ€»æ–‡ä»¶æ ¼å¼
    legacy_files = [
        data_path / f"papers_{date_str}_100percent.json",
        data_path / f"papers_{date_str}.json",
    ]

    for json_file in legacy_files:
        if json_file.exists():
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                    if isinstance(data, list):
                        # ç›´æ¥æ˜¯è®ºæ–‡åˆ—è¡¨
                        all_papers = data
                    elif isinstance(data, dict):
                        # åŒ…å« metadata çš„æ ¼å¼
                        if "papers" in data:
                            all_papers = data["papers"]
                        else:
                            # å¯èƒ½æ˜¯å•ä¸ªè®ºæ–‡å¯¹è±¡ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
                            all_papers = [data]
                    else:
                        st.warning(f"Unexpected data format in {json_file}")
                        continue
                        
                    st.success(f"âœ… Loaded {len(all_papers)} papers from legacy file {json_file}")
                    return all_papers

            except json.JSONDecodeError as e:
                st.error(f"Invalid JSON in {json_file}: {e}")
                continue
            except Exception as e:
                st.error(f"Error loading papers from {json_file}: {e}")
                continue
    
    # æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ä»¶
    return []


def filter_papers_by_categories(papers: List[Dict], categories: List[str]) -> List[Dict]:
    """æ ¹æ®é€‰æ‹©çš„åˆ†ç±»è¿‡æ»¤è®ºæ–‡"""
    if not categories:
        return papers

    # è½¬æ¢åˆ†ç±»åç§°ä¸ºä»£ç 
    category_codes = [ARXIV_CATEGORIES.get(cat, cat) for cat in categories]

    filtered = []
    for paper in papers:
        paper_categories = paper.get("categories", [])
        if isinstance(paper_categories, str):
            paper_categories = [paper_categories]

        # æ£€æŸ¥è®ºæ–‡æ˜¯å¦å±äºä»»ä¸€é€‰ä¸­çš„åˆ†ç±»
        if any(cat in paper_categories for cat in category_codes):
            filtered.append(paper)

    return filtered


def find_matching_terms(query: str, title: str, abstract: str) -> Dict[str, List[str]]:
    """
    æ‰¾åˆ°ä¸æŸ¥è¯¢åŒ¹é…çš„å…³é”®è¯ï¼ˆæ”¯æŒ stemmingï¼‰

    Args:
        query: æœç´¢æŸ¥è¯¢
        title: è®ºæ–‡æ ‡é¢˜
        abstract: è®ºæ–‡æ‘˜è¦

    Returns:
        åŒ…å«åŒ¹é…å…³é”®è¯çš„å­—å…¸
    """
    if not query or not query.strip():
        return {"title": [], "abstract": []}

    try:
        import tantivy

        # åˆ›å»ºä¸æœç´¢ç´¢å¼•ç›¸åŒçš„ stemmer analyzer
        tokenizer = tantivy.Tokenizer.whitespace()
        stemmer_filter = tantivy.Filter.stemmer('english')
        stemmer_analyzer = tantivy.TextAnalyzerBuilder(tokenizer).filter(stemmer_filter).build()

        # å°†æŸ¥è¯¢åˆ†å‰²ä¸ºå…³é”®è¯å¹¶è¿›è¡Œ stemming
        query_terms = re.findall(r'\b\w+\b', query.lower())
        if not query_terms:
            query_terms = [query.lower()]

        # ä¸ºæ¯ä¸ªæŸ¥è¯¢è¯åˆ›å»ºè¯å¹²æ˜ å°„
        query_stems = {}
        for term in query_terms:
            try:
                stemmed = list(stemmer_analyzer.analyze(term))
                if stemmed:
                    stem = stemmed[0]  # å–ç¬¬ä¸€ä¸ªè¯å¹²
                    if stem not in query_stems:
                        query_stems[stem] = []
                    query_stems[stem].append(term)
            except:
                # å¦‚æœ stemming å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹è¯
                stem = term
                if stem not in query_stems:
                    query_stems[stem] = []
                query_stems[stem].append(term)

    except ImportError:
        # å¦‚æœ tantivy ä¸å¯ç”¨ï¼Œå›é€€åˆ°ç®€å•åŒ¹é…
        query_stems = {term: [term] for term in re.findall(r'\b\w+\b', query.lower()) or [query.lower()]}

    matching_title_terms = []
    matching_abstract_terms = []

    # ä¸ºæ ‡é¢˜ä¸­çš„æ¯ä¸ªè¯è¿›è¡Œ stemmingï¼Œæ£€æŸ¥æ˜¯å¦åŒ¹é…æŸ¥è¯¢è¯å¹²
    title_words = re.findall(r'\b\w+\b', title)
    for word in title_words:
        try:
            stemmed = list(stemmer_analyzer.analyze(word.lower()))
            if stemmed and stemmed[0] in query_stems:
                # æ‰¾åˆ°åŒ¹é…ï¼Œæ·»åŠ åŸå§‹å¤§å°å†™çš„è¯
                matching_title_terms.append(word)
        except:
            # å¦‚æœ stemming å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦ç›´æ¥åŒ¹é…
            if word.lower() in [t.lower() for terms in query_stems.values() for t in terms]:
                matching_title_terms.append(word)

    # ä¸ºæ‘˜è¦ä¸­çš„æ¯ä¸ªè¯è¿›è¡Œ stemmingï¼Œæ£€æŸ¥æ˜¯å¦åŒ¹é…æŸ¥è¯¢è¯å¹²
    abstract_words = re.findall(r'\b\w+\b', abstract)
    for word in abstract_words:
        try:
            stemmed = list(stemmer_analyzer.analyze(word.lower()))
            if stemmed and stemmed[0] in query_stems:
                # æ‰¾åˆ°åŒ¹é…ï¼Œæ·»åŠ åŸå§‹å¤§å°å†™çš„è¯
                matching_abstract_terms.append(word)
        except:
            # å¦‚æœ stemming å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦ç›´æ¥åŒ¹é…
            if word.lower() in [t.lower() for terms in query_stems.values() for t in terms]:
                matching_abstract_terms.append(word)

    # å»é‡
    matching_title_terms = list(set(matching_title_terms))
    matching_abstract_terms = list(set(matching_abstract_terms))

    return {
        "title": matching_title_terms,
        "abstract": matching_abstract_terms
    }

def highlight_text(text: str, terms: List[str], highlight_color: str = "#FFFF00") -> str:
    """
    åœ¨æ–‡æœ¬ä¸­é«˜äº®åŒ¹é…çš„å…³é”®è¯

    Args:
        text: åŸå§‹æ–‡æœ¬
        terms: è¦é«˜äº®çš„å…³é”®è¯åˆ—è¡¨
        highlight_color: é«˜äº®é¢œè‰²

    Returns:
        åŒ…å«é«˜äº®æ ‡è®°çš„HTMLæ–‡æœ¬
    """
    if not terms or not text:
        return text

    # è½¬ä¹‰HTMLç‰¹æ®Šå­—ç¬¦
    text = str(text)

    # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œé«˜äº®
    for term in terms:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å•è¯è¾¹ç•Œ
        pattern = r'\b(' + re.escape(term) + r')\b'
        text = re.sub(pattern, f'<mark style="background-color: {highlight_color}; padding: 0 2px; border-radius: 2px;">\\1</mark>', text, flags=re.IGNORECASE)

    return text


def search_papers_simple(query: str, papers: List[Dict], categories: Optional[List[str]] = None) -> List[Dict]:
    """
    åœ¨è®ºæ–‡ä¸­æœç´¢ï¼ˆæœç´¢æ ‡é¢˜å’Œæ‘˜è¦ï¼‰
    ç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…å®ç°

    Args:
        query: æœç´¢å…³é”®è¯
        papers: è®ºæ–‡åˆ—è¡¨
        categories: åˆ†ç±»è¿‡æ»¤åˆ—è¡¨

    Returns:
        æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªè®ºæ–‡åŒ…å«åŒ¹é…å…³é”®è¯ä¿¡æ¯
    """
    if not query:
        return papers

    query_lower = query.lower()
    results = []

    for paper in papers:
        title = paper.get("title", "").lower()
        abstract = paper.get("abstract", "").lower()

        # ç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…
        if query_lower in title or query_lower in abstract:
            # åº”ç”¨åˆ†ç±»è¿‡æ»¤ï¼ˆå¦‚æœæŒ‡å®šäº†åˆ†ç±»ï¼‰
            if categories:
                paper_categories = paper.get("categories", [])
                if isinstance(paper_categories, str):
                    paper_categories = [paper_categories]

                if not any(cat in paper_categories for cat in categories):
                    continue

            # åˆ›å»ºè®ºæ–‡å‰¯æœ¬å¹¶æ·»åŠ åŒ¹é…å…³é”®è¯ä¿¡æ¯
            paper_with_matches = paper.copy()
            paper_with_matches["_search_matches"] = find_matching_terms(query, paper.get("title", ""), paper.get("abstract", ""))
            results.append(paper_with_matches)

    return results


def search_papers(query: str, papers: List[Dict], categories: Optional[List[str]] = None) -> List[Dict]:
    """
    æœç´¢è®ºæ–‡ - æ™ºèƒ½é€‰æ‹©æœç´¢æ–¹å¼

    Args:
        query: æœç´¢å…³é”®è¯
        papers: è®ºæ–‡åˆ—è¡¨
        categories: åˆ†ç±»è¿‡æ»¤

    Returns:
        æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªè®ºæ–‡åŒ…å«åŒ¹é…å…³é”®è¯ä¿¡æ¯
    """
    if not query or not query.strip():
        return papers

    # å¦‚æœ BM25 æœç´¢å¼•æ“å¯ç”¨ï¼Œä¼˜å…ˆä½¿ç”¨
    if SEARCH_ENGINE_AVAILABLE and st.session_state.search_mode == "bm25":
        try:
            # åˆå§‹åŒ–æˆ–è·å–æœç´¢å¼•æ“
            if st.session_state.search_engine is None:
                st.session_state.search_engine = PaperSearchEngine()

            # ä½¿ç”¨ BM25 æœç´¢
            results = search_papers_bm25(
                query=query,
                papers=papers,
                categories=categories,
                search_engine=st.session_state.search_engine,
                rebuild_index=True  # æ¯æ¬¡éƒ½é‡å»ºç´¢å¼•ï¼Œç¡®ä¿åªæœç´¢å½“å‰è®ºæ–‡
            )

            # ä¸ºBM25æœç´¢ç»“æœæ·»åŠ åŒ¹é…å…³é”®è¯ä¿¡æ¯
            for paper in results:
                if "_search_matches" not in paper:
                    paper["_search_matches"] = find_matching_terms(query, paper.get("title", ""), paper.get("abstract", ""))

            return results

        except Exception as e:
            st.warning(f"âš ï¸ BM25 æœç´¢å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æœç´¢: {e}")
            return search_papers_simple(query, papers, categories)
    else:
        # ä½¿ç”¨ç®€å•æœç´¢
        return search_papers_simple(query, papers, categories)


def render_category_pills(categories: List[str]):
    """æ¸²æŸ“ Pills èƒ¶å›Šå¼åˆ†ç±»æ ‡ç­¾ - ä½¿ç”¨StreamlitåŸç”Ÿç»„ä»¶"""

    # åˆ›å»ºèƒ¶å›ŠHTML
    pills_html = ""
    for cat in categories:
        colors = CATEGORY_COLORS.get(cat, {"bg": "#F0F0F0", "border": "#BDBDBD", "text": "#424242"})
        pills_html += f'<span style="background-color:{colors["bg"]};color:{colors["text"]};border:2px solid {colors["border"]};padding:6px 12px;border-radius:15px;font-size:12px;font-weight:500;margin:0 4px 4px 0;display:inline-block;">ğŸ”– {cat}</span>'

    st.markdown(f'<div style="margin:10px 0;">{pills_html}</div>', unsafe_allow_html=True)

def papers_to_csv(papers: List[Dict]) -> str:
    """
    å°†è®ºæ–‡åˆ—è¡¨è½¬æ¢ä¸ºCSVå­—ç¬¦ä¸²

    Args:
        papers: è®ºæ–‡åˆ—è¡¨

    Returns:
        CSVæ ¼å¼çš„å­—ç¬¦ä¸²
    """
    if not papers:
        return ""

    # å®šä¹‰CSVåˆ—
    columns = ['title', 'authors', 'categories', 'published_date', 'abstract', 'url', 'pdf_url']

    # åˆ›å»ºæ•°æ®è¡Œ
    data = []
    for paper in papers:
        row = {
            'title': paper.get('title', ''),
            'authors': ', '.join(paper.get('authors', [])) if isinstance(paper.get('authors'), list) else paper.get('authors', ''),
            'categories': ', '.join(paper.get('categories', [])) if isinstance(paper.get('categories'), list) else paper.get('categories', ''),
            'published_date': paper.get('published_date', ''),
            'abstract': paper.get('abstract', ''),
            'url': paper.get('url', ''),
            'pdf_url': paper.get('pdf_url', '')
        }
        data.append(row)

    # è½¬æ¢ä¸ºDataFrameç„¶åå¯¼å‡ºä¸ºCSV
    df = pd.DataFrame(data, columns=columns)
    return df.to_csv(index=False)


def render_paper_card(paper: Dict):

    """æ¸²æŸ“å•ä¸ªè®ºæ–‡å¡ç‰‡"""

    # è®ºæ–‡å®¹å™¨
    with st.container():
        # è·å–æœç´¢åŒ¹é…ä¿¡æ¯
        search_matches = paper.get("_search_matches", {"title": [], "abstract": []})

        # æ ‡é¢˜
        title = paper.get("title", "Untitled")
        url = paper.get("url", "") or paper.get("pdf_url", "")

        # é«˜äº®æ ‡é¢˜ä¸­çš„åŒ¹é…å…³é”®è¯
        highlighted_title = highlight_text(title, search_matches.get("title", []))

        # æ˜¾ç¤ºæ ‡é¢˜
        if url:
            # å¦‚æœæœ‰é“¾æ¥ï¼Œä½¿ç”¨HTMLæ¥ç¡®ä¿é«˜äº®å’Œé“¾æ¥éƒ½æ­£å¸¸å·¥ä½œ
            st.markdown(f'<h3><a href="{url}" style="text-decoration: none; color: inherit;">{highlighted_title}</a></h3>', unsafe_allow_html=True)
        else:
            st.markdown(f'<h3>{highlighted_title}</h3>', unsafe_allow_html=True)

        # ä½œè€…
        authors = paper.get("authors", [])
        if authors:
            if isinstance(authors, list):
                if len(authors) > 5:
                    author_str = ", ".join(authors[:5]) + " et al."
                else:
                    author_str = ", ".join(authors)
            else:
                author_str = str(authors)
            st.markdown(f"**ğŸ‘¥ Authors:** {author_str}")

        # åˆ†ç±»å’Œå‘å¸ƒæ—¥æœŸ
        col1, col2 = st.columns(2)
        with col1:
            categories = paper.get("categories", [])
            if categories:
                if isinstance(categories, list):
                    categories_str = ", ".join(categories[:3])
                else:
                    categories_str = str(categories)
                st.caption(f"ğŸ·ï¸ Categories: {categories_str}")
        with col2:
            pub_date = paper.get("published_date")
            if pub_date:
                st.caption(f"ğŸ“… Published: {pub_date}")

        # æ‘˜è¦
        abstract = paper.get("abstract", "")
        if abstract:
            st.markdown("#### ğŸ“„ Abstract")
            # é«˜äº®æ‘˜è¦ä¸­çš„åŒ¹é…å…³é”®è¯
            highlighted_abstract = highlight_text(abstract, search_matches.get("abstract", []))
            st.markdown(highlighted_abstract, unsafe_allow_html=True)

        # é“¾æ¥æŒ‰é’®
        col1, _ = st.columns([1, 4])

        with col1:
            pdf_url = paper.get("pdf_url", "")
            if pdf_url:
                st.link_button("ğŸ“„ PDF", pdf_url)


        # åˆ†å‰²çº¿
        st.divider()



        # åˆ†å‰²çº¿
        st.divider()

def main():
    """ä¸»åº”ç”¨"""
    
    # è‡ªå®šä¹‰ CSS
    st.markdown("""
    <style>
    div[data-testid="stExpander"] {
        border: 1px solid #ddd;
        border-radius: 5px;
        padding: 10px;
        margin: 10px 0;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # ä¾§è¾¹æ  - ArXiv åˆ†ç±»é€‰æ‹©
    with st.sidebar:
        st.title("ğŸ“š Cool Papers")
        st.caption("Simplified Interface")
        
        st.markdown("---")
        
        # ArXiv åˆ†ç±»é€‰æ‹©
        st.subheader("ğŸ”¬ arXiv Categories")
        st.caption("Select your interested categories")
        
        selected_cats = []
        for cat_name, cat_code in ARXIV_CATEGORIES.items():
            is_selected = cat_code in st.session_state.selected_categories
            if st.checkbox(cat_name, value=is_selected, key=f"cat_{cat_code}"):
                selected_cats.append(cat_code)
        
        st.session_state.selected_categories = selected_cats
        
        st.markdown("---")
        
        # æ˜¾ç¤ºé€‰ä¸­çš„åˆ†ç±»æ•°é‡
        st.metric("ğŸ“‚ Selected Categories", len(st.session_state.selected_categories))
        
        st.markdown("---")
        
        # å…³äº
        st.caption("**About**")
        st.caption("Cool Papers - Simplified Interface")
        st.caption("Data loaded from local JSON files")
    
    # ä¸»é¡µé¢
    st.header("arxiv è®ºæ–‡åŒæ­¥")

    st.markdown("---")

    # æ—¥æœŸå’Œåˆ†ç±»å¹¶æ’æ˜¾ç¤º
    date_col, cat_col = st.columns([1, 3])

    with date_col:
        st.caption("é€‰æ‹©æ—¥æœŸ")
        selected_date = st.date_input(
            "Select a date to view papers",
            value=datetime.now(),
            max_value=datetime.now(),
            min_value=datetime.now() - timedelta(days=365),
            key="date_picker",
            label_visibility="collapsed"
        )

    with cat_col:
        st.caption("ç±»åˆ«")
        if st.session_state.selected_categories:
            render_category_pills(st.session_state.selected_categories)
        else:
            st.warning("âš ï¸ Please select at least one category from the sidebar")

    date_str = selected_date.strftime("%Y-%m-%d")

    st.markdown("---")

    # æœç´¢åŒºåŸŸ - å•ç‹¬ä¸€è¡Œ
    st.header("æœç´¢")

    search_col1, search_col2, export_col = st.columns([4, 1, 1])
    with search_col1:
        search_query = st.text_input(
            "Search Query",
            placeholder="è¾“å…¥å…³é”®è¯ æˆ–è€… ä»€ä¹ˆéƒ½ä¸è¾“å…¥",
            label_visibility="collapsed",
            key="search_box"
        )

    with search_col2:
        search_button = st.button("Search", type="primary", use_container_width=True)

    # å¯¼å‡ºæŒ‰é’®ä¼šåœ¨è¿™é‡ŒåŠ¨æ€æ·»åŠ ï¼ˆå½“æœ‰è®ºæ–‡æ—¶ï¼‰
    
    st.markdown("---")
    
    # åŠ è½½å¹¶æ˜¾ç¤ºè®ºæ–‡
    if st.session_state.selected_categories:
        with st.spinner(f"Loading papers for {date_str}..."):
            # åŠ è½½è®ºæ–‡
            papers = load_papers_from_json(date_str, st.session_state.selected_categories)

            if not papers:
                st.warning(f"ğŸ“­ No papers found for date {date_str}")
            else:
                # è®ºæ–‡å·²ç»æŒ‰é€‰ä¸­çš„ç±»åˆ«åŠ è½½ï¼Œæ— éœ€é¢å¤–è¿‡æ»¤
                # ç¡®å®šè¦æ˜¾ç¤ºçš„è®ºæ–‡åˆ—è¡¨
                if search_query and search_query.strip():
                    # åœ¨åŠ è½½çš„è®ºæ–‡ä¸­æœç´¢
                    display_papers = search_papers(search_query, papers, st.session_state.selected_categories)

                    if not display_papers:
                        st.info(f"No results found for '{search_query}'")
                    else:
                        st.success(f"Found {len(display_papers)} papers matching '{search_query}'")
                else:
                    # æ˜¾ç¤ºæ€»åŠ è½½è®ºæ–‡æ•°é‡
                    st.success(f"âœ… Loaded {len(papers)} papers")
                    display_papers = papers

                # åœ¨æœç´¢åŒºåŸŸæ·»åŠ å¯¼å‡ºæŒ‰é’®
                if display_papers:
                    with export_col:
                        # ä½¿ç”¨base64ç¼–ç é¿å…åª’ä½“æ–‡ä»¶ç¼“å­˜é—®é¢˜
                        import base64
                        csv_data = papers_to_csv(display_papers)
                        b64_data = base64.b64encode(csv_data.encode()).decode()

                        # ä½¿ç”¨HTMLä¸‹è½½é“¾æ¥é¿å…Streamlitåª’ä½“æ–‡ä»¶ç¼“å­˜
                        download_link = f'<a href="data:text/csv;base64,{b64_data}" download="papers_{date_str}.csv" style="text-decoration: none;"><button style="background-color: #FF6B6B; color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer; width: 100%;">Export CSV</button></a>'

                        st.markdown(download_link, unsafe_allow_html=True)

                # æ˜¾ç¤ºè®ºæ–‡åˆ—è¡¨
                for paper in display_papers:
                    render_paper_card(paper)
    
    # é¡µè„š
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; color: #666;">
        <p><strong>Cool Papers</strong> - Simplified Streamlit Interface</p>
        <p>Data loaded from local JSON files</p>
    </div>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()
