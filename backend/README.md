# Cool Papers Backend

å®Œæ•´çš„åç«¯å®ç°ï¼Œç”¨äº **Cool Papers** æ²‰æµ¸å¼è®ºæ–‡å‘ç°å¹³å°ã€‚

## åŠŸèƒ½ç‰¹æ€§

### ğŸ•·ï¸ å¤šæºè®ºæ–‡çˆ¬è™«
- **ArXiv**: æ”¯æŒæ‰€æœ‰åˆ†ç±»ï¼Œå®æ—¶è·å–æœ€æ–°è®ºæ–‡
- **OpenReview**: ä¼šè®®è®ºæ–‡è·å–
- **ACL Anthology**: ACLç³»åˆ—ä¼šè®®è®ºæ–‡
- **PMLR**: æœºå™¨å­¦ä¹ ä¼šè®®è®ºæ–‡é›†

### ğŸ“Š æ•°æ®ç®¡ç†
- SQLAlchemy ORM å¼‚æ­¥æ•°æ®åº“æ“ä½œ
- æ”¯æŒ SQLiteï¼ˆå¼€å‘ï¼‰å’Œ PostgreSQLï¼ˆç”Ÿäº§ï¼‰
- è®ºæ–‡å…ƒæ•°æ®å­˜å‚¨å’Œæ£€ç´¢
- ç”¨æˆ·æ´»åŠ¨è¿½è¸ª

### ğŸ” å…¨æ–‡æœç´¢
- åŸºäº Tantivy çš„ BM25 æœç´¢å¼•æ“
- æ”¯æŒæ ‡é¢˜ã€æ‘˜è¦ã€å…¨æ–‡æœç´¢
- åˆ†ç±»å’Œä¼šè®®è¿‡æ»¤
- æœç´¢å»ºè®®åŠŸèƒ½

### ğŸ“„ PDF å¤„ç†
- è‡ªåŠ¨ä» PDF æå–å…¨æ–‡
- PDF ç¼“å­˜æœºåˆ¶
- æ”¯æŒå¤šç§è®ºæ–‡æºçš„ PDF

### ğŸ“¡ API æ¥å£
- RESTful API è®¾è®¡
- è‡ªåŠ¨ç”Ÿæˆ API æ–‡æ¡£ï¼ˆSwaggerï¼‰
- RSS/Atom Feed æ”¯æŒ
- CORS æ”¯æŒ

### âš¡ æ€§èƒ½ä¼˜åŒ–
- å¼‚æ­¥ I/O (asyncio + httpx)
- è¯·æ±‚é€Ÿç‡é™åˆ¶
- æ•°æ®ç¼“å­˜
- æ‰¹é‡æ“ä½œä¼˜åŒ–

## æŠ€æœ¯æ ˆ

- **Web Framework**: FastAPI
- **Database**: SQLAlchemy (async) + SQLite/PostgreSQL
- **Search Engine**: Tantivy (BM25)
- **PDF Processing**: PyMuPDF
- **HTTP Client**: httpx, aiohttp
- **HTML Parsing**: PyQuery, BeautifulSoup
- **Feed Generation**: feedgen

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
cd backend
pip install -r requirements.txt
```

### 2. é…ç½®ç¯å¢ƒå˜é‡

```bash
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œé…ç½®æ•°æ®åº“ã€APIå¯†é’¥ç­‰
```

### 3. åˆå§‹åŒ–æ•°æ®åº“

æ•°æ®åº“ä¼šåœ¨é¦–æ¬¡å¯åŠ¨æ—¶è‡ªåŠ¨åˆå§‹åŒ–ã€‚

### 4. å¯åŠ¨æœåŠ¡

```bash
python main.py
```

æˆ–ä½¿ç”¨ uvicornï¼š

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### 5. è®¿é—® API æ–‡æ¡£

æ‰“å¼€æµè§ˆå™¨è®¿é—®ï¼š
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

## API ç«¯ç‚¹

### è®ºæ–‡æ“ä½œ

- `GET /papers/arxiv/{paper_id}` - è·å–å•ç¯‡ ArXiv è®ºæ–‡
- `GET /papers/arxiv/list/{category}` - è·å–åˆ†ç±»è®ºæ–‡åˆ—è¡¨
- `GET /papers/arxiv/combined` - è·å–å¤šåˆ†ç±»ç»„åˆï¼ˆæ”¯æŒæ’é™¤ï¼‰
- `GET /papers/venue/{venue_id}` - è·å–ä¼šè®®è®ºæ–‡
- `GET /papers/{source}/{paper_id}` - è·å–ä»»æ„æºè®ºæ–‡
- `POST /papers/{paper_id}/click` - è®°å½•ç‚¹å‡»ç»Ÿè®¡
- `GET /papers/{paper_id}/full_text` - æå– PDF å…¨æ–‡

### æœç´¢

- `GET /search/` - å…¨æ–‡æœç´¢
- `GET /search/arxiv` - ArXiv API æœç´¢
- `GET /search/suggestions` - æœç´¢å»ºè®®

### RSS/Atom Feeds

- `GET /feeds/arxiv/{category}` - ArXiv åˆ†ç±»è®¢é˜…
- `GET /feeds/venue/{venue_id}` - ä¼šè®®è®¢é˜…
- `GET /feeds/latest` - æœ€æ–°è®ºæ–‡è®¢é˜…

## é¡¹ç›®ç»“æ„

```
backend/
â”œâ”€â”€ main.py                 # FastAPI åº”ç”¨å…¥å£
â”œâ”€â”€ config.py              # é…ç½®ç®¡ç†
â”œâ”€â”€ database.py            # æ•°æ®åº“è¿æ¥
â”œâ”€â”€ models.py              # SQLAlchemy æ¨¡å‹
â”œâ”€â”€ requirements.txt       # Python ä¾èµ–
â”œâ”€â”€ .env.example          # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â”œâ”€â”€ api/                  # API è·¯ç”±
â”‚   â”œâ”€â”€ papers.py         # è®ºæ–‡ç›¸å…³æ¥å£
â”‚   â”œâ”€â”€ search.py         # æœç´¢æ¥å£
â”‚   â””â”€â”€ feeds.py          # Feed è®¢é˜…æ¥å£
â”œâ”€â”€ scrapers/             # çˆ¬è™«æ¨¡å—
â”‚   â”œâ”€â”€ base_scraper.py   # çˆ¬è™«åŸºç±»
â”‚   â”œâ”€â”€ arxiv_scraper.py  # ArXiv çˆ¬è™«
â”‚   â”œâ”€â”€ openreview_scraper.py
â”‚   â”œâ”€â”€ acl_scraper.py
â”‚   â””â”€â”€ pmlr_scraper.py
â””â”€â”€ utils/                # å·¥å…·æ¨¡å—
    â”œâ”€â”€ pdf_processor.py  # PDF å¤„ç†
    â””â”€â”€ search_engine.py  # æœç´¢å¼•æ“
```

## ä½¿ç”¨ç¤ºä¾‹

### è·å– ArXiv è®ºæ–‡

```python
import httpx

# è·å–å•ç¯‡è®ºæ–‡
response = httpx.get("http://localhost:8000/papers/arxiv/2401.12345")
paper = response.json()

# è·å–åˆ†ç±»åˆ—è¡¨
response = httpx.get("http://localhost:8000/papers/arxiv/list/cs.AI")
papers = response.json()

# å¤šåˆ†ç±»ç»„åˆï¼ˆcs.AI æˆ– cs.LGï¼Œä½†æ’é™¤ cs.CYï¼‰
response = httpx.get(
    "http://localhost:8000/papers/arxiv/combined",
    params={"include": "cs.AI,cs.LG", "exclude": "cs.CY"}
)
```

### æœç´¢è®ºæ–‡

```python
# å…¨æ–‡æœç´¢
response = httpx.get(
    "http://localhost:8000/search/",
    params={"query": "transformer attention", "max_results": 50}
)
results = response.json()
```

### è®¢é˜… Feed

åœ¨ RSS é˜…è¯»å™¨ä¸­æ·»åŠ ï¼š
- `http://localhost:8000/feeds/arxiv/cs.AI`
- `http://localhost:8000/feeds/venue/ICML.2024`
- `http://localhost:8000/feeds/latest`

## éƒ¨ç½²

### ä½¿ç”¨ Docker

åˆ›å»º `Dockerfile`:

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

æ„å»ºå¹¶è¿è¡Œï¼š

```bash
docker build -t coolpapers-backend .
docker run -p 8000:8000 -v ./data:/app/data coolpapers-backend
```

### ä½¿ç”¨ Docker Compose

åˆ›å»º `docker-compose.yml`:

```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql+asyncpg://user:pass@db:5432/coolpapers
    depends_on:
      - db
    volumes:
      - ./data:/app/data
  
  db:
    image: postgres:15
    environment:
      POSTGRES_DB: coolpapers
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

è¿è¡Œï¼š

```bash
docker-compose up -d
```

### ç”Ÿäº§ç¯å¢ƒå»ºè®®

1. **ä½¿ç”¨ PostgreSQL** è€Œä¸æ˜¯ SQLite
2. **é…ç½® Redis** ç”¨äºç¼“å­˜
3. **ä½¿ç”¨ Nginx** ä½œä¸ºåå‘ä»£ç†
4. **å¯ç”¨ HTTPS** (Let's Encrypt)
5. **é…ç½®æ—¥å¿—è½®è½¬**
6. **è®¾ç½®ç›‘æ§å’Œå‘Šè­¦** (Prometheus + Grafana)
7. **ä½¿ç”¨ Gunicorn + Uvicorn Workers**:

```bash
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000
```

## å®šæ—¶ä»»åŠ¡

ä¸ºäº†è‡ªåŠ¨æ›´æ–°è®ºæ–‡ï¼Œå¯ä»¥è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼ˆcronï¼‰ï¼š

```python
# scripts/update_arxiv.py
import asyncio
from datetime import datetime
from scrapers import ArxivScraper
from database import AsyncSessionLocal
from models import Paper

async def update_arxiv_papers():
    """æ›´æ–° ArXiv è®ºæ–‡"""
    scraper = ArxivScraper()
    categories = ["cs.AI", "cs.LG", "cs.CL", "cs.CV"]
    
    async with AsyncSessionLocal() as db:
        for category in categories:
            papers = await scraper.fetch_latest(category)
            for paper_data in papers:
                paper = Paper(**paper_data)
                db.add(paper)
        await db.commit()

if __name__ == "__main__":
    asyncio.run(update_arxiv_papers())
```

Crontab é…ç½®ï¼ˆæ¯å¤©æ—©ä¸Š 8 ç‚¹æ›´æ–°ï¼‰ï¼š

```bash
0 8 * * * cd /path/to/backend && python scripts/update_arxiv.py
```

## æ€§èƒ½ä¼˜åŒ–

1. **æ•°æ®åº“ç´¢å¼•**: å·²åœ¨æ¨¡å‹ä¸­å®šä¹‰å…³é”®ç´¢å¼•
2. **æ‰¹é‡æ“ä½œ**: ä½¿ç”¨ `add_papers_batch` æ‰¹é‡æ’å…¥
3. **ç¼“å­˜**: å¯ç”¨ Redis ç¼“å­˜çƒ­é—¨æŸ¥è¯¢
4. **å¼‚æ­¥æ“ä½œ**: æ‰€æœ‰ I/O æ“ä½œä½¿ç”¨ async/await
5. **è¿æ¥æ± **: é…ç½®æ•°æ®åº“è¿æ¥æ± å¤§å°

## æ•…éšœæ’æŸ¥

### æ•°æ®åº“è¿æ¥é”™è¯¯
```bash
# æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æƒé™
ls -la coolpapers.db

# é‡æ–°åˆå§‹åŒ–æ•°æ®åº“
rm coolpapers.db
python main.py
```

### ArXiv çˆ¬å–å¤±è´¥
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- ç¡®è®¤ ArXiv æ˜¯å¦å¯è®¿é—®
- è°ƒæ•´ `ARXIV_RATE_LIMIT` é¿å…è¢«é™æµ

### æœç´¢ç´¢å¼•é—®é¢˜
```bash
# æ¸…é™¤å¹¶é‡å»ºç´¢å¼•
rm -rf search_index/
# é‡å¯åº”ç”¨ä¼šè‡ªåŠ¨é‡å»º
```

## å¼€å‘

### è¿è¡Œæµ‹è¯•

```bash
pytest tests/
```

### ä»£ç æ ¼å¼åŒ–

```bash
black .
isort .
```

### ç±»å‹æ£€æŸ¥

```bash
mypy .
```

## è®¸å¯è¯

MIT License

## è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## è”ç³»æ–¹å¼

- GitHub: https://github.com/yourusername/coolpapers
- Email: your.email@example.com
