"""
import requests
import xml.etree.ElementTree as ET
100% Complete Data Fetching Strategy for arXiv Papers
100% å®Œæ•´æ•°æ®è·å–ç­–ç•¥

This script GUARANTEES 100% data completeness by:
1. Continuous retry until all papers are fetched
2. Incremental fetching (track what's already fetched)
3. Completeness verification (compare with total_results)
4. Resume from checkpoint on restart
5. Multiple verification passes

Time is not a constraint - it will keep trying until complete.
"""

import asyncio
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
import os
from src.config import ArxivSettings
from src.services.arxiv.client import ArxivClient
from src.schemas.arxiv.paper import ArxivPaper

from src.scripts.scrape_arxiv_passweek_and_parse import fetch_pass_week_papers
from collections import defaultdict, OrderedDict

from Bio import Entrez, Medline
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ArXiv categories
ARXIV_CATEGORIES = {
    "Artificial Intelligence (cs.AI)": "cs.AI",
    "Computation and Language (cs.CL)": "cs.CL",
    "Computer Vision (cs.CV)": "cs.CV",
    "Machine Learning (cs.LG)": "cs.LG",
    "Neural and Evolutionary Computing (cs.NE)": "cs.NE",
    "Computational Complexity (cs.CC)": "cs.CC",
    "Statistics - Machine Learning (stat.ML)": "stat.ML",
}

# Configuration
DEFAULT_OUTPUT_DIR = "./papers_data"
CHECKPOINT_DIR = "./checkpoints"  # Store progress
MAX_RETRY_WAIT_SECONDS = 300  # Max 5 minutes between retries
VERIFICATION_PASSES = 3  # Number of verification passes
LOCAL_FILE_PATH = "./papers_data/"

class CompleteFetcher:
    """Guarantees 100% complete data fetching."""

    def __init__(
        self,
        output_dir: str = DEFAULT_OUTPUT_DIR,
        checkpoint_dir: str = CHECKPOINT_DIR,
        categories: Optional[List[str]] = None,
    ):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        self.categories = categories or list(ARXIV_CATEGORIES.values())
        
        logger.info(f"Initialized CompleteFetcher (100% guarantee mode)")
        logger.info(f"Categories: {self.categories}")
        logger.info(f"Checkpoints: {self.checkpoint_dir}")

    def _get_checkpoint_file(self, category: str, date: str) -> Path:
        """Get checkpoint file path."""
        return self.checkpoint_dir / f"checkpoint_{category}_{date}.json"

    def _load_checkpoint(self, category: str, date: str) -> Dict:
        """Load checkpoint data."""
        checkpoint_file = self._get_checkpoint_file(category, date)
        if checkpoint_file.exists():
            try:
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Failed to load checkpoint: {e}")
        return {
            "fetched_ids": [],
            "fetched_papers": [],  # å®Œæ•´çš„è®ºæ–‡æ•°æ®
            "total_expected": None,
            "attempts": 0,
            "last_attempt": None,
        }

    def _save_checkpoint(self, category: str, date: str, checkpoint: Dict):
        """Save checkpoint data."""
        checkpoint_file = self._get_checkpoint_file(category, date)
        checkpoint["last_attempt"] = datetime.now().isoformat()
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint, f, indent=2)

    def _clear_checkpoint(self, category: str, date: str):
        """Clear checkpoint after successful completion."""
        checkpoint_file = self._get_checkpoint_file(category, date)
        if checkpoint_file.exists():
            checkpoint_file.unlink()
            logger.info(f"[{category}] Checkpoint cleared")
    
    async def async_daily_pubmed(self, date: Optional[datetime] = None):
        Entrez.email = "mzthhy@hotmail.com"
        CORE_QUERY = '((LLM) OR (VLM)) OR (Scan Protocol)'

        # Determine the date to fetch - either specified date or yesterday
        if date is not None:
            target_date = date
            logger.info(f"start to scrape Pubmed for date: {target_date.strftime('%Y-%m-%d')}")
        else:
            # Default to yesterday's date for the past 24 hours
            target_date = datetime.now() - timedelta(days=1)
            logger.info(f"start to scrape Pubmed for yesterday: {target_date.strftime('%Y-%m-%d')}")

        async def fetch_daily_updates():
            fetch_results = defaultdict(list)
            metadata = {}

            max_wait_hours = 1

            start_time = datetime.now()
            max_wait_seconds = 1 * 3600
            attempt_count = 0
            while True:
                attempt_count+=1
                elapsed = (datetime.now() - start_time).total_seconds()
                logger.info(f"[PubMed] Attempt #{attempt_count} (elapsed: {elapsed/3600:.1f}h)")
                if elapsed > max_wait_seconds:
                    logger.error(f"Max wait time ({max_wait_hours}h) exceeded. ")
                    return fetch_results, metadata

                try:
                    # Always use date range parameters for precise control
                    # Format date as YYYY/MM/DD for PubMed API
                    mindate = target_date.strftime("%Y/%m/%d")
                    maxdate = (target_date + timedelta(days=1)).strftime("%Y/%m/%d")
                    logger.info(f"Searching PubMed with date range: {mindate} to {maxdate}")
                    handle = Entrez.esearch(
                        db="pubmed", 
                        term=CORE_QUERY, 
                        mindate=mindate,
                        maxdate=maxdate,
                        datetype="edat",  # å‚æ•°ï¼šæ—¥æœŸç±»å‹å¿…é¡»è®¾ä¸º edat (Entry Date)
                        retmax=100
                    )
                    record = Entrez.read(handle)
                    handle.close()
                    
                    id_list = record["IdList"]
                    count = record["Count"]

                    if not id_list:
                        logger.info(f"âœ… {target_date.strftime('%Y-%m-%d')} æ²¡æœ‰å‘ç°æ–°è®ºæ–‡ã€‚")
                        return

                    logger.info(f"ğŸš€ å‘ç° {count} ç¯‡æ–°è®ºæ–‡ï¼å‡†å¤‡è·å–è¯¦æƒ…...\n")
                    
                    handle = Entrez.efetch(db="pubmed", id=id_list, rettype="medline", retmode="text")

                    records = Medline.parse(handle)

            # 3. éå†è§£æ
                    for i, record in enumerate(records):

                        # --- æå–ä¿¡æ¯ ---
                        title = record.get("TI", "No Title")
                        pmid = record.get("PMID", "No PMID")
                        published_date = record.get("EDAT", "No Published Date")
                        doi = record.get("LID", "No DOI")
                        # [å…³é”®] ç›´æ¥è·å–å®Œæ•´æ‘˜è¦ (Medline åº“ä¼šè‡ªåŠ¨æ‹¼æ¥å¤šè¡Œ)
                        abstract = record.get("AB", "No Abstract")
                        # æå–ä½œè€…
                        authors = record.get("AU", ["Unknown Author"])
                        # æå–åˆ†ç±»æ ‡ç­¾
                        journal = record.get("JT", "Unknown Journal") # æœŸåˆŠ
                        pub_types = record.get("PT", [])             # æ–‡ç« ç±»å‹
                        categories = (f"[{journal}] " + "".join([f"[{pt}]" for pt in pub_types if pt != 'Journal Article']))
                        # æå–å…³é”®è¯ (ä¼˜å…ˆå–ä½œè€…å…³é”®è¯ OTï¼Œæ²¡æœ‰åˆ™å– MeSH)
                        keywords = record.get("OT", [])
                        if not keywords:
                            # å¦‚æœæ²¡æœ‰ä½œè€…å…³é”®è¯ï¼Œå°è¯•ä» MH ä¸­æå–å¸¦æ˜Ÿå·çš„æ ¸å¿ƒè¯
                            mesh = record.get("MH", [])
                            keywords = [m.replace("*", "") for m in mesh if "*" in m]

                        # æå– DOI (ä½äº LID æˆ– AID å­—æ®µ)
                        doi = ""
                        if "LID" in record:
                            # LID æ ¼å¼é€šå¸¸æ˜¯ "10.xxx [doi]"
                            doi = next((x.replace(" [doi]", "") for x in record["LID"] if "[doi]" in x), "")
                        elif "AID" in record:
                            doi = next((x.replace(" [doi]", "") for x in record["AID"] if "[doi]" in x), "")

                        # --- æ‰“å°æ¼‚äº®çš„è¾“å‡º ---
                        logger.info(f"ã€{i+1}ã€‘ {title}")
                        logger.info(f"ğŸ·ï¸  åˆ†ç±»: [{journal}] " + "".join([f"[{pt}]" for pt in pub_types if pt != 'Journal Article']))
                        logger.info(f"ğŸ‘¥ ä½œè€…: {', '.join(authors[:5])}" + ("..." if len(authors)>5 else ""))
                        if keywords:
                            logger.info(f"ğŸ”‘ å…³é”®è¯: {', '.join(keywords)}")
                        logger.info(f"DP: {published_date}")
                        
                        logger.info(f"\nğŸ“– å®Œæ•´æ‘˜è¦:")
                        logger.info(f"{abstract}") # è¿™é‡Œè¾“å‡ºçš„å°±æ˜¯å®Œæ•´çš„ä¸€å¤§æ®µè¯
                        
                        logger.info(f"\nğŸ”— PubMed: https://pubmed.ncbi.nlm.nih.gov/ {record.get('PMID', '?')}/")
                        if doi:
                            logger.info(f"ğŸ”— DOIç›´è¾¾: https://doi.org/ {doi}")
                            
                        logger.info("=" * 60) # åˆ†å‰²çº¿

                        try:
                            # EDAT å­—æ®µå¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—ç¬¦ä¸²ï¼Œéœ€è¦å…ˆå¤„ç†
                            if isinstance(published_date, list):
                                published_date = published_date[0] if published_date else "No Published Date"
                            
                            # æ¸…ç†æ—¥æœŸå­—ç¬¦ä¸²ï¼Œç§»é™¤å¯èƒ½çš„é¢å¤–ç©ºæ ¼
                            published_date = published_date.strip()
                            
                            # EDAT æ ¼å¼: "YYYY/MM/DD HH:MM" æˆ– "YYYY/MM/DD"
                            if "/" in published_date:
                                # ç§»é™¤å¯èƒ½çš„æ—¶é—´éƒ¨åˆ† (HH:MM)
                                date_part = published_date.split()[0]  # å–ç©ºæ ¼å‰çš„æ—¥æœŸéƒ¨åˆ†
                                paper_date = datetime.strptime(date_part, "%Y/%m/%d")
                            else:
                                # å…¼å®¹å…¶ä»–å¯èƒ½çš„æ ¼å¼
                                logger.warning(f"Unexpected EDAT format: {published_date}, skipping")
                                continue
                        except (ValueError, IndexError) as e:
                            logger.warning(f"Failed to parse EDAT '{published_date}': {e}, skipping")
                            continue

                        # Only include papers from the target date
                        #if paper_date.date() != target_date.date():
                        #    logger.info(f"Skipping paper {title} from {paper_date.strftime('%Y-%m-%d')} because it is not the target date {target_date.strftime('%Y-%m-%d')}")
                        #    continue

                        fetch_results[paper_date].append({
                            "title": title,
                            "arxiv_id": pmid,
                            "published_date": paper_date.strftime("%Y-%m-%d"),
                            "doi": doi,
                            "abstract": abstract,
                            "authors": authors,
                            "categories": categories,
                            'pdf_url': f"https://pubmed.ncbi.nlm.nih.gov/ {pmid}/",
                        })
                        if paper_date not in metadata:
                            metadata[paper_date] = {
                                "expected_total": 1,
                            }
                        else:
                            metadata[paper_date]["expected_total"] += 1

                    handle.close()
                    return fetch_results, metadata

                except Exception as e:
                    logger.error(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

                # Wait before retry (with exponential backoff)
                retry_delay = min(retry_delay * 1.5, MAX_RETRY_WAIT_SECONDS)
                logger.info(f"Error fetching daily PubMed updates: Waiting {retry_delay:.0f}s before next attempt...")
                import time
                time.sleep(retry_delay)
            
        fetch_results, metadata = await fetch_daily_updates()
        sorted_fetch_results = OrderedDict(sorted(fetch_results.items(), key=lambda item: item[0], reverse=True))
        sorted_metadata = OrderedDict(sorted(metadata.items(), key=lambda item: item[0], reverse=True))
        papers_by_date = defaultdict(list)
        metadata_by_date = defaultdict(list)
        for date, results in sorted_fetch_results.items():
            try:
                for result in results:
                    papers_by_date[date].append(result)
                    metadata_by_date[date].append(sorted_metadata[date])
            except Exception as e:
                import pdb;pdb.set_trace()
                logger.error(f"Error processing date {date}: {e}")
                continue
        for date, papers in papers_by_date.items():
            # ä¸ºæ¯ä¸ªæ—¥æœŸåˆ›å»ºç‹¬ç«‹çš„åˆ†ç±»æ•°æ®ç»“æ„
            paper_by_category = defaultdict(list)
            metadata_by_category = defaultdict(list)

            # åªå¤„ç†å½“å‰æ—¥æœŸçš„è®ºæ–‡
            for paper in papers:
                paper_by_category["PubMed"].append(paper)

            # ä¸ºå½“å‰æ—¥æœŸè®¾ç½®metadata
            metadata_by_category["PubMed"] = {
                "expected_total": metadata[date]["expected_total"],
                "is_complete": True,
                "total_attempts": 0,
                "elapsed_hours": 0
            }

            self.save_papers_with_metadata(paper_by_category, metadata_by_category, date)


    async def async_daily_scrape(self, target_date=None):

        to_do_list = defaultdict(dict)
        overall_groups = {}
        overall_groups_ = defaultdict(dict)
        for category in self.categories:
            groups = fetch_pass_week_papers(category)
            overall_groups[category] = groups

        for category, groups in overall_groups.items():
            for date, papers in groups.items():
                dt = datetime.strptime(date, "%a, %d %b %Y")
                overall_groups_[date][category] = []
                for paper in papers:
                    overall_groups_[date][category].append(paper)
        
        # ä»fetchç»“æœä¸­æ‰¾åˆ°æœ€æ–°çš„æ—¥æœŸï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰
        all_dates = list(overall_groups_.keys())
        if all_dates:
            # æŒ‰æ—¥æœŸæ’åºï¼Œæ‰¾åˆ°æœ€æ–°çš„
            sorted_dates = sorted(all_dates, key=lambda x: datetime.strptime(x, "%a, %d %b %Y"), reverse=True)
            latest_date = sorted_dates[0]  # æœ€æ–°çš„æ—¥æœŸ
            old_dates = sorted_dates[1:]   # å…¶ä½™çš„æ—§æ—¥æœŸ

            logger.info(f"Processing latest date from fetch: {latest_date}")
            logger.info(f"Checking existence for old dates: {old_dates}")

            # å¦‚æœæŒ‡å®šäº†ç›®æ ‡æ—¥æœŸï¼Œåªå¤„ç†è¯¥æ—¥æœŸï¼Œå¦åˆ™å¤„ç†æœ€æ–°æ—¥æœŸ
            if target_date:
                # å°†ç›®æ ‡æ—¥æœŸè½¬æ¢ä¸ºç›¸åŒçš„æ ¼å¼è¿›è¡ŒåŒ¹é…
                target_date_str = target_date.strftime("%a, %d %b %Y")
                if target_date_str in overall_groups_:
                    dates_to_process = [target_date_str]
                    logger.info(f"Processing specified date: {target_date_str}")
                else:
                    logger.warning(f"Specified date {target_date_str} not found in available dates")
                    dates_to_process = []
            else:
                # é»˜è®¤åªå¤„ç†æœ€æ–°çš„æ•°æ®
                dates_to_process = sorted_dates
                logger.info(f"Processing sorted date: {sorted_dates}")

            # å¤„ç†é€‰å®šçš„æ—¥æœŸ
            for date in dates_to_process:
                category_dict = overall_groups_[date]
                dt = datetime.strptime(date, "%a, %d %b %Y")

                # ä¸ºè¿™ä¸ªæ—¥æœŸæ”¶é›†æ‰€æœ‰å·²å­˜åœ¨çš„è®ºæ–‡IDï¼ŒæŒ‰ç±»åˆ«åˆ†ç»„
                existing_papers_by_category = {}
                for category in self.categories:
                    paper_file = os.path.join(LOCAL_FILE_PATH,
                        category, f"papers_{dt.strftime('%Y-%m-%d')}_100percent.json")
                    if os.path.exists(paper_file):
                        with open(paper_file, 'r', encoding='utf-8') as f:
                            papers_scraped = json.load(f)
                        papers_lists = papers_scraped['papers']
                        existing_papers_by_category[category] = set(paper['arxiv_id'] for paper in papers_lists)
                    else:
                        existing_papers_by_category[category] = set()

                for category, paper_ids in category_dict.items():
                    to_do_list[date][category] = []
                    existing_papers = existing_papers_by_category[category]

                    for paper in paper_ids:
                        paper_id_with_version = f"{paper['id']}v{paper['version']}"
                        # åªæœ‰å½“è¯¥ç±»åˆ«è¿˜æ²¡æœ‰è¿™ç¯‡è®ºæ–‡æ—¶ï¼Œæ‰æ·»åŠ åˆ°æŠ“å–åˆ—è¡¨
                        if paper_id_with_version not in existing_papers:
                            to_do_list[date][category].append(paper_id_with_version)

            # æ£€æŸ¥æ—§æ•°æ®çš„å­˜åœ¨æ€§ï¼Œä½†ä¸è¿›è¡ŒæŠ“å–
            for date_str in old_dates:
                dt = datetime.strptime(date_str, "%a, %d %b %Y")
                logger.info(f"Checking existence for old date: {date_str}")

                for category in self.categories:
                    paper_file = os.path.join(LOCAL_FILE_PATH,
                        category, f"papers_{dt.strftime('%Y-%m-%d')}_100percent.json")
                    if os.path.exists(paper_file):
                        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å®Œæ•´ï¼ˆæœ‰metadataä¸”is_completeä¸ºtrueï¼‰
                        try:
                            with open(paper_file, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                            metadata = data.get('metadata', {})
                            if metadata.get('is_complete', False):
                                logger.info(f"[{category}] {date_str}: File exists and marked complete âœ“")
                            else:
                                logger.warning(f"[{category}] {date_str}: File exists but incomplete âš ï¸")
                        except Exception as e:
                            logger.warning(f"[{category}] {date_str}: Error reading file: {e}")
                    else:
                        logger.warning(f"[{category}] {date_str}: File missing âŒ")

        papers_by_date = {}
        metadata_by_date = {}

        for date_str, category_dict in to_do_list.items():
            # Convert string date to datetime object
            date_obj = datetime.strptime(date_str, "%a, %d %b %Y")
            papers_by_date[date_obj.strftime('%Y-%m-%d')] = []
            metadata_by_date[date_obj.strftime('%Y-%m-%d')] = []

            for category, paper_id_list in category_dict.items():
                logger.info(f"[{category}] Starting 100% complete daily fetch for {date_obj.strftime('%Y-%m-%d')}")
                papers, metadata = await self.fetch_papers_by_id(date_obj.strftime('%Y-%m-%d'), category, paper_id_list, preserve_order=True)
                papers_by_date[date_obj.strftime('%Y-%m-%d')].extend(papers)
                metadata_by_date[date_obj.strftime('%Y-%m-%d')].append(metadata)

        return papers_by_date, metadata_by_date
        
    async def fetch_papers_by_id(self, date, category, paper_id_list, max_wait_hours=24, preserve_order=False):

        start_time = datetime.now()
        max_wait_seconds = max_wait_hours * 3600

        # Load checkpoint
        checkpoint = self._load_checkpoint(category, date)
        fetched_ids: Set[str] = set(checkpoint["fetched_ids"])
        fetched_papers = checkpoint.get("fetched_papers", [])
        total_expected = checkpoint["total_expected"]
        attempt_count = checkpoint["attempts"]

        logger.info(f"[{category}] Starting 100% complete fetch")
        logger.info(f"[{category}] Checkpoint: {len(fetched_ids)} papers already fetched")
        if total_expected:
            logger.info(f"[{category}] Expected total: {total_expected}")

        if total_expected is None:
            total_expected = len(paper_id_list)

        # ä»checkpointæ¢å¤å·²è·å–çš„è®ºæ–‡
        all_papers_dict = {}  # Use dict to avoid duplicates
        for paper_data in fetched_papers:
            # å°†å­—å…¸æ•°æ®è½¬æ¢å›ArxivPaperå¯¹è±¡
            paper = ArxivPaper(**paper_data)
            all_papers_dict[paper.arxiv_id] = paper

        logger.info(f"[{category}] Restored {len(all_papers_dict)} papers from checkpoint")

        retry_delay = 10  # Initial retry delay
        consecutive_failures = 0

        while True:
            elapsed = (datetime.now() - start_time).total_seconds()
            if elapsed > max_wait_seconds:
                logger.error(
                    f"[{category}] Max wait time ({max_wait_hours}h) exceeded. "
                    f"Fetched {len(all_papers_dict)}/{total_expected or '?'} papers"
                )
                break

            attempt_count += 1
            logger.info(f"[{category}] Attempt #{attempt_count} (elapsed: {elapsed/3600:.1f}h)")

            try:
                # è®¡ç®—å“ªäº›IDè¿˜æ²¡æœ‰è·å–
                remaining_ids = [pid for pid in paper_id_list if pid not in fetched_ids]
                if not remaining_ids:
                    logger.info(f"[{category}] All {total_expected} papers already fetched from checkpoint")
                    break

                logger.info(f"[{category}] Fetching {len(remaining_ids)} remaining papers")

                # Create client
                settings = ArxivSettings(search_category=category)
                client = ArxivClient(settings)

                # åªè·å–å‰©ä½™çš„è®ºæ–‡
                papers = await client.fetch_papers_by_ids(
                    arxiv_ids=remaining_ids
                )

                # æŒ‰ç…§è¾“å…¥çš„paper_id_listé¡ºåºé‡æ–°æ’åˆ—ç»“æœ
                ordered_papers = []
                papers_dict = {paper.arxiv_id.split("v")[0]: paper for paper in papers}  # ä½¿ç”¨æ¸…ç†åçš„IDä½œä¸ºkey
                for paper_id in remaining_ids:
                    clean_id = paper_id.split("v")[0] if "v" in paper_id else paper_id
                    if clean_id in papers_dict:
                        ordered_papers.append(papers_dict[clean_id])

                new_papers = 0
                # Add newly fetched papers
                for paper in ordered_papers:
                    if paper.arxiv_id not in all_papers_dict:
                        all_papers_dict[paper.arxiv_id] = paper
                        fetched_ids.add(paper.arxiv_id)
                        new_papers += 1

                logger.info(
                    f"[{category}] Fetched {len(ordered_papers)} papers this attempt "
                    f"({new_papers} new, {len(all_papers_dict)} total)"
                )

                # Check completeness
                if total_expected and len(all_papers_dict) >= total_expected:
                    logger.info(
                        f"[{category}] âœ“ COMPLETE! Fetched {len(all_papers_dict)}/{total_expected} papers"
                    )
                    consecutive_failures = 0
                    break  # Success!
                elif total_expected:
                    missing = total_expected - len(all_papers_dict)
                    logger.warning(
                        f"[{category}] Incomplete: {len(all_papers_dict)}/{total_expected} "
                        f"({missing} missing, {len(all_papers_dict)/total_expected*100:.1f}% complete)"
                    )
                else:
                    logger.info(f"[{category}] Fetched {len(all_papers_dict)} papers (total unknown)")

                # æ›´æ–°checkpointä¸­çš„è®ºæ–‡æ•°æ®
                fetched_papers_data = []
                for paper in all_papers_dict.values():
                    # å°†ArxivPaperå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸ä»¥ä¾¿JSONåºåˆ—åŒ–
                    paper_dict = {
                        "arxiv_id": paper.arxiv_id,
                        "title": paper.title,
                        "authors": paper.authors,
                        "abstract": paper.abstract,
                        "categories": paper.categories,
                        "published_date": paper.published_date,
                        "pdf_url": paper.pdf_url,
                    }
                    fetched_papers_data.append(paper_dict)

                # Save checkpoint
                checkpoint["fetched_ids"] = list(fetched_ids)
                checkpoint["fetched_papers"] = fetched_papers_data
                checkpoint["total_expected"] = total_expected
                checkpoint["attempts"] = attempt_count
                self._save_checkpoint(category, date, checkpoint)

                # Reset retry delay on successful fetch (even if incomplete)
                if new_papers > 0:
                    retry_delay = 5
                    consecutive_failures = 0
                else:
                    consecutive_failures += 1
                    # If no new papers for multiple attempts, might be complete
                    if consecutive_failures >= VERIFICATION_PASSES:
                        if total_expected is None or len(all_papers_dict) > 0:
                            logger.info(
                                f"[{category}] No new papers after {consecutive_failures} verification passes. "
                                f"Considering complete with {len(all_papers_dict)} papers."
                            )
                            break

            except Exception as e:
                logger.error(f"[{category}] Attempt #{attempt_count} failed: {e}")
                consecutive_failures += 1

            # Wait before retry (with exponential backoff)
            if not (total_expected and len(all_papers_dict) >= total_expected):
                retry_delay = min(retry_delay * 1.5, MAX_RETRY_WAIT_SECONDS)
                logger.info(f"[{category}] Waiting {retry_delay:.0f}s before next attempt...")
                await asyncio.sleep(retry_delay)
        
        # Convert to simplified format
        simplified_papers = []
        if preserve_order:
            # æŒ‰ç…§è¾“å…¥çš„paper_id_listé¡ºåºä¿å­˜
            for paper_id in paper_id_list:
                clean_id = paper_id.split("v")[0] if "v" in paper_id else paper_id
                # æ‰¾åˆ°å¯¹åº”çš„å®Œæ•´IDï¼ˆåŒ…å«ç‰ˆæœ¬å·ï¼‰
                matching_key = None
                for key in all_papers_dict.keys():
                    if key.split("v")[0] == clean_id:
                        matching_key = key
                        break

                if matching_key:
                    paper = all_papers_dict[matching_key]
                    simplified_papers.append({
                        "arxiv_id": paper.arxiv_id,
                        "title": paper.title,
                        "authors": paper.authors,
                        "abstract": paper.abstract,
                        "categories": paper.categories,
                        "published_date": paper.published_date,
                        "url": f"https://arxiv.org/abs/ {paper.arxiv_id}",
                        "pdf_url": paper.pdf_url,
                        "source_category": category,  # æ·»åŠ æºç±»åˆ«å­—æ®µ
                    })
                else:
                    logger.warning(f"[{category}] Paper {clean_id} not found in fetched papers")
                # å¦‚æœæ‰¾ä¸åˆ°åŒ¹é…çš„ï¼Œè·³è¿‡ï¼ˆä¿æŒè¾“å…¥é¡ºåºï¼Œåªæ·»åŠ å­˜åœ¨çš„è®ºæ–‡ï¼‰
        else:
            # é»˜è®¤é¡ºåºï¼Œç›´æ¥éå†æ‰€æœ‰è·å–åˆ°çš„è®ºæ–‡
            for paper in all_papers_dict.values():
                simplified_papers.append({
                    "arxiv_id": paper.arxiv_id,
                    "title": paper.title,
                    "authors": paper.authors,
                    "abstract": paper.abstract,
                    "categories": paper.categories,
                    "published_date": paper.published_date,
                    "url": f"https://arxiv.org/abs/ {paper.arxiv_id}",
                    "pdf_url": paper.pdf_url,
                    "source_category": category,  # æ·»åŠ æºç±»åˆ«å­—æ®µ
                })

        # Build metadata
        is_complete = total_expected is None or len(simplified_papers) >= total_expected
        metadata = {
            "category": category,
            "date_range": f"{date}",
            "total_attempts": attempt_count,
            "elapsed_hours": (datetime.now() - start_time).total_seconds() / 3600,
            "papers_fetched": len(simplified_papers),
            "expected_total": total_expected,
            "completeness": "100%" if is_complete else f"{len(simplified_papers)/total_expected*100:.1f}%",
            "is_complete": is_complete,
        }

        # Clear checkpoint on success
        if is_complete:
            self._clear_checkpoint(category, date)

        return simplified_papers, metadata

    async def fetch_category_complete(
        self,
        category: str,
        from_date: str,
        to_date: str,
        max_wait_hours: int = 24,  # Maximum total wait time
    ) -> Tuple[List[Dict], Dict]:
        """
        Fetch papers for a category with 100% completeness guarantee.
        
        Will keep retrying until:
        1. All expected papers are fetched, OR
        2. max_wait_hours is exceeded (safety limit)
        
        Args:
            category: arXiv category code
            from_date: Start date (YYYYMMDD)
            to_date: End date (YYYYMMDD)
            max_wait_hours: Maximum hours to keep trying (default: 24)
            
        Returns:
            Tuple of (papers, metadata) where metadata includes completeness info
        """
        start_time = datetime.now()
        max_wait_seconds = max_wait_hours * 3600
        
        # Load checkpoint
        checkpoint = self._load_checkpoint(category, from_date)
        fetched_ids: Set[str] = set(checkpoint["fetched_ids"])
        total_expected = checkpoint["total_expected"]
        attempt_count = checkpoint["attempts"]
        
        logger.info(f"[{category}] Starting 100% complete fetch")
        logger.info(f"[{category}] Checkpoint: {len(fetched_ids)} papers already fetched")
        if total_expected:
            logger.info(f"[{category}] Expected total: {total_expected}")
        
        all_papers_dict = {}  # Use dict to avoid duplicates
        retry_delay = 10  # Initial retry delay
        consecutive_failures = 0
        
        while True:
            elapsed = (datetime.now() - start_time).total_seconds()
            if elapsed > max_wait_seconds:
                logger.error(
                    f"[{category}] Max wait time ({max_wait_hours}h) exceeded. "
                    f"Fetched {len(all_papers_dict)}/{total_expected or '?'} papers"
                )
                break
            
            attempt_count += 1
            logger.info(f"[{category}] Attempt #{attempt_count} (elapsed: {elapsed/3600:.1f}h)")
            
            try:
                # Create client
                settings = ArxivSettings(search_category=category)
                client = ArxivClient(settings)
                
                # Fetch all papers with unlimited retries per page
                papers, results = await client.fetch_all_papers_in_date_range(
                    from_date=from_date,
                    to_date=to_date,
                    max_per_page=100,
                    sort_by="submittedDate",
                    sort_order="descending",
                    max_retries_per_page=10,  # More retries per page
                )
                
                # Update expected total
                if results and len(results) > 0:
                    new_total = results[0].total_results
                    if total_expected is None:
                        total_expected = new_total
                        logger.info(f"[{category}] Total expected papers: {total_expected}")
                    elif new_total != total_expected:
                        logger.warning(
                            f"[{category}] Total changed: {total_expected} â†’ {new_total}"
                        )
                        total_expected = new_total
                
                # Add newly fetched papers
                new_papers = 0
                for paper in papers:
                    if paper.arxiv_id not in all_papers_dict and paper.arxiv_id not in fetched_ids:
                        all_papers_dict[paper.arxiv_id] = paper
                        fetched_ids.add(paper.arxiv_id)
                        new_papers += 1
                
                logger.info(
                    f"[{category}] Fetched {len(papers)} papers this attempt "
                    f"({new_papers} new, {len(all_papers_dict)} total)"
                )
                
                # Check completeness
                if total_expected and len(all_papers_dict) >= total_expected:
                    logger.info(
                        f"[{category}] âœ“ COMPLETE! Fetched {len(all_papers_dict)}/{total_expected} papers"
                    )
                    consecutive_failures = 0
                    break  # Success!
                elif total_expected:
                    missing = total_expected - len(all_papers_dict)
                    logger.warning(
                        f"[{category}] Incomplete: {len(all_papers_dict)}/{total_expected} "
                        f"({missing} missing, {len(all_papers_dict)/total_expected*100:.1f}% complete)"
                    )
                else:
                    logger.info(f"[{category}] Fetched {len(all_papers_dict)} papers (total unknown)")
                
                # Save checkpoint
                checkpoint["fetched_ids"] = list(fetched_ids)
                checkpoint["total_expected"] = total_expected
                checkpoint["attempts"] = attempt_count
                self._save_checkpoint(category, from_date, checkpoint)
                
                # Reset retry delay on successful fetch (even if incomplete)
                if new_papers > 0:
                    retry_delay = 10
                    consecutive_failures = 0
                else:
                    consecutive_failures += 1
                    # If no new papers for multiple attempts, might be complete
                    if consecutive_failures >= VERIFICATION_PASSES:
                        if total_expected is None or len(all_papers_dict) > 0:
                            logger.info(
                                f"[{category}] No new papers after {consecutive_failures} verification passes. "
                                f"Considering complete with {len(all_papers_dict)} papers."
                            )
                            break
                
            except Exception as e:
                logger.error(f"[{category}] Attempt #{attempt_count} failed: {e}")
                consecutive_failures += 1
                
            # Wait before retry (with exponential backoff)
            if not (total_expected and len(all_papers_dict) >= total_expected):
                retry_delay = min(retry_delay * 1.5, MAX_RETRY_WAIT_SECONDS)
                logger.info(f"[{category}] Waiting {retry_delay:.0f}s before next attempt...")
                await asyncio.sleep(retry_delay)
        
        # Convert to simplified format
        simplified_papers = []
        for paper in all_papers_dict.values():
            simplified_papers.append({
                "arxiv_id": paper.arxiv_id,
                "title": paper.title,
                "authors": paper.authors,
                "abstract": paper.abstract,
                "categories": paper.categories,
                "published_date": paper.published_date,
                "url": f"https://arxiv.org/abs/ {paper.arxiv_id}",
                "pdf_url": paper.pdf_url,
                "source_category": category,  # æ·»åŠ æºç±»åˆ«å­—æ®µ
            })
        
        # Build metadata
        is_complete = total_expected is None or len(simplified_papers) >= total_expected
        metadata = {
            "category": category,
            "date_range": f"{from_date}-{to_date}",
            "total_attempts": attempt_count,
            "elapsed_hours": (datetime.now() - start_time).total_seconds() / 3600,
            "papers_fetched": len(simplified_papers),
            "expected_total": total_expected,
            "completeness": "100%" if is_complete else f"{len(simplified_papers)/total_expected*100:.1f}%",
            "is_complete": is_complete,
        }
        
        # Clear checkpoint on success
        if is_complete:
            self._clear_checkpoint(category, from_date)
        
        return simplified_papers, metadata

    async def fetch_date_complete(
        self,
        date: datetime,
        categories: Optional[List[str]] = None,
        max_wait_hours: int = 24,
    ) -> Tuple[Dict[str, List[Dict]], Dict[str, Dict]]:
        """
        Fetch papers for all categories with 100% completeness guarantee.
        
        Args:
            date: Date to fetch papers for
            categories: List of categories (default: all)
            max_wait_hours: Max hours per category
            
        Returns:
            Tuple of (papers_by_category, metadata_by_category)
        """
        date_str = date.strftime("%Y%m%d")
        categories_to_fetch = categories or self.categories
        
        logger.info("=" * 80)
        logger.info(f"Starting 100% COMPLETE fetch for {date.strftime('%Y-%m-%d')}")
        logger.info(f"Categories: {len(categories_to_fetch)}")
        logger.info(f"Max wait per category: {max_wait_hours}h")
        logger.info("=" * 80)
        
        # Fetch each category sequentially (to avoid overwhelming API)
        papers_by_category = {}
        metadata_by_category = {}
        
        for i, category in enumerate(categories_to_fetch, 1):
            logger.info(f"\n{'='*80}")
            logger.info(f"Category {i}/{len(categories_to_fetch)}: {category}")
            logger.info(f"{'='*80}")
            
            papers, metadata = await self.fetch_category_complete(
                category=category,
                from_date=date_str,
                to_date=date_str,
                max_wait_hours=max_wait_hours,
            )
            
            papers_by_category[category] = papers
            metadata_by_category[category] = metadata
            
            if metadata["is_complete"]:
                logger.info(f"[{category}] âœ… 100% COMPLETE: {metadata['papers_fetched']} papers")
            else:
                logger.warning(
                    f"[{category}] âš ï¸ INCOMPLETE: {metadata['papers_fetched']}/{metadata['expected_total']} "
                    f"({metadata['completeness']})"
                )
        
        return papers_by_category, metadata_by_category

    def save_papers_with_metadata(
        self,
        papers_by_category: Dict[str, List[Dict]],
        metadata_by_category: Dict[str, Dict],
        date: datetime,
    ) -> List[Path]:
        """Save papers by category with detailed completeness metadata."""
        date_str = date.strftime("%Y-%m-%d")
        saved_files = []

        # Save each category separately
        for category, papers in papers_by_category.items():
            category_dir = self.output_dir / category
            category_dir.mkdir(parents=True, exist_ok=True)

            output_file = category_dir / f"papers_{date_str}_100percent.json"
            metadata = metadata_by_category.get(category, {})

            # Build output for this category
            output_data = {
                "metadata": {
                    "fetch_mode": "100_percent_complete",
                    "fetch_date": datetime.now().isoformat(),
                    "paper_date": date_str,
                    "category": category,
                    "total_papers": len(papers),
                    "expected_total": metadata.get("expected_total", 0),
                    "completeness": metadata.get("completeness", "unknown"),
                    "is_complete": metadata.get("is_complete", False),
                    "total_attempts": metadata.get("total_attempts", 0),
                    "elapsed_hours": metadata.get("elapsed_hours", 0),
                },
                "papers": papers,
            }

            # Save
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)

            saved_files.append(output_file)

            # Log completion status for this category
            if metadata.get("is_complete"):
                logger.info(f"âœ… [{category}] 100% COMPLETE: {len(papers)} papers saved to {output_file}")
            else:
                logger.warning(f"âš ï¸ [{category}] INCOMPLETE: {len(papers)}/{metadata.get('expected_total', '?')} papers ({metadata.get('completeness', 'unknown')}) saved to {output_file}")

        # Overall summary
        total_papers = sum(len(papers) for papers in papers_by_category.values())
        all_complete = all(meta.get("is_complete", False) for meta in metadata_by_category.values())

        if all_complete:
            logger.info("=" * 80)
            logger.info(f"âœ… ALL CATEGORIES 100% COMPLETE SUCCESS!")
            logger.info(f"âœ… Total saved {total_papers} papers across {len(papers_by_category)} categories")
            logger.info("=" * 80)
        else:
            incomplete_cats = [
                cat for cat, meta in metadata_by_category.items()
                if not meta.get("is_complete", False)
            ]
            logger.warning("=" * 80)
            logger.warning(f"âš ï¸ SOME CATEGORIES INCOMPLETE: {len(incomplete_cats)} categories not 100% complete")
            for cat in incomplete_cats:
                meta = metadata_by_category[cat]
                logger.warning(f"  - {cat}: {meta.get('completeness', 'unknown')}")
            logger.warning(f"âš ï¸ Total saved {total_papers} papers across {len(papers_by_category)} categories")
            logger.warning("=" * 80)

        return saved_files

    async def run_daily_complete(
        self,
        date: Optional[datetime] = None,
        max_wait_hours: int = 24,
    ):
        """
        Run complete fetch for all available dates from past week.

        Args:
            date: If specified, only fetch this date. If None, fetch all available dates.
            max_wait_hours: Max hours to spend per category
        """

        # Fetch all categories and dates
        papers_by_date, metadata_by_date = await self.async_daily_scrape(target_date=date)

        # If a specific date is requested and not found, log it
        if date is not None:
            date_str = date.strftime('%Y-%m-%d')
            if date_str not in papers_by_date:
                logger.info(f"No papers found for date {date_str}")
                papers_by_date = {}
                metadata_by_date = {}
            
            ## Also fetch PubMed for the specified date
            #await self.async_daily_pubmed(date=date)

        saved_files = []
        for current_date_str, papers in papers_by_date.items():
            current_date = datetime.strptime(current_date_str, "%Y-%m-%d")
            metadata_list = metadata_by_date[current_date_str]

            papers_by_category = {}
            metadata_by_category = {}

            for paper in papers:
                # ä½¿ç”¨source_categoryåˆ†ç»„ï¼Œä¿æŒåŸå§‹æŠ“å–é¡ºåº
                source_category = paper.get("source_category", paper["categories"][0] if paper["categories"] else "unknown")
                if source_category not in papers_by_category:
                    papers_by_category[source_category] = []
                papers_by_category[source_category].append(paper)

            # Group metadata by category
            for metadata in metadata_list:
                category = metadata["category"]
                metadata_by_category[category] = metadata

            # Save results for this date (by category)
            category_files = self.save_papers_with_metadata(papers_by_category, metadata_by_category, current_date)
            saved_files.extend(category_files)

        if not saved_files:
            logger.warning(f"No data found to save")
            return None

        return saved_files[0] if len(saved_files) == 1 else saved_files

    async def run_continuous_complete(
        self,
        check_interval_hours: int = 24,
        max_wait_per_category: int = 12,
    ):
        """
        Run continuously, ensuring 100% completeness for each day.
        
        Args:
            check_interval_hours: Hours between checks
            max_wait_per_category: Max hours per category per attempt
        """
        logger.info("=" * 80)
        logger.info("Starting CONTINUOUS 100% COMPLETE mode")
        logger.info(f"Check interval: {check_interval_hours}h")
        logger.info(f"Max wait per category: {max_wait_per_category}h")
        logger.info("=" * 80)
        
        while True:
            try:
                # Fetch yesterday (most reliable)
                yesterday = datetime.now() - timedelta(days=1)
                await self.run_daily_complete(
                    date=yesterday,
                    max_wait_hours=max_wait_per_category,
                )
                
                # Also check day before yesterday (in case it was incomplete)
                two_days_ago = datetime.now() - timedelta(days=2)
                await self.run_daily_complete(
                    date=two_days_ago,
                    max_wait_hours=max_wait_per_category,
                )
                
                # Wait before next check
                logger.info(f"\nWaiting {check_interval_hours}h until next check...")
                await asyncio.sleep(check_interval_hours * 3600)
                
            except KeyboardInterrupt:
                logger.info("Received interrupt signal, stopping...")
                break
            except Exception as e:
                logger.error(f"Error in continuous mode: {e}")
                logger.info("Retrying in 1 hour...")
                await asyncio.sleep(3600)


    async def run_fetch_by_custom_ids(
        self,
        custom_id_list: List[str],
        output_file: Optional[str] = None,
        max_wait_hours: int = 24,
    ):
        """
        ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„IDåˆ—è¡¨è·å–è®ºæ–‡ï¼Œå¹¶æŒ‰ç…§åˆ—è¡¨é¡ºåºä¿å­˜

        Args:
            custom_id_list: ç”¨æˆ·æŒ‡å®šçš„arxiv IDåˆ—è¡¨
            category: åˆ†ç±»ï¼ˆç”¨äºAPIè®¾ç½®ï¼‰
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™è‡ªåŠ¨ç”Ÿæˆ
            max_wait_hours: æœ€å¤§ç­‰å¾…æ—¶é—´
        """
        logger.info("=" * 80)
        logger.info("Starting CUSTOM ID LIST fetch")
        logger.info(f"Category: custom_list_mode")
        logger.info(f"Total IDs: {len(custom_id_list)}")
        logger.info("=" * 80)

        # ä½¿ç”¨fetch_papers_by_idæ–¹æ³•è·å–è®ºæ–‡ï¼Œä¿æŒé¡ºåº
        papers, metadata = await self.fetch_papers_by_id(
            date="1900-01-01",
            category="custom_list_mode",
            paper_id_list=custom_id_list,
            max_wait_hours=max_wait_hours,
            preserve_order=False
        )

        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if output_file is None:
            timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            output_file = self.output_dir / f"papers_custom_list_{timestamp}.json"

        # ä¿å­˜è®ºæ–‡
        output_data = {
            "metadata": {
                "fetch_mode": "custom_id_list",
                "fetch_date": datetime.now().isoformat(),
                "category": "custom_list_mode",
                "total_papers": len(papers),
                "expected_total": len(custom_id_list),
                "preserved_order": False,
            },
            "papers": papers,
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        logger.info("=" * 80)
        logger.info(f"âœ… Saved {len(papers)} papers to {output_file}")
        logger.info("=" * 80)

        return output_file


async def main():
    """Main entry point."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Fetch arXiv papers with 100% completeness guarantee"
    )
    parser.add_argument(
        "--date",
        type=str,
        help="Fetch papers for specific date (YYYY-MM-DD). If not specified, fetch all available dates from past week"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=DEFAULT_OUTPUT_DIR,
        help="Output directory"
    )
    parser.add_argument(
        "--categories",
        type=str,
        nargs="+",
        help="Categories to fetch"
    )
    parser.add_argument(
        "--max-wait-hours",
        type=int,
        default=24,
        help="Maximum hours to wait per category (default: 24)"
    )
    parser.add_argument(
        "--continuous",
        action="store_true",
        help="Run continuously"
    )
    parser.add_argument(
        "--check-interval",
        type=int,
        default=24,
        help="Hours between checks in continuous mode (default: 24)"
    )
    parser.add_argument(
        "--Fetch-Type",
        type=str,
        choices=['arXiv', 'PubMed', 'all'],
        default='all',
        help="Type of fetch (default: all)"
    )

    args = parser.parse_args()
    
    # Create fetcher
    fetcher = CompleteFetcher(
        output_dir=args.output_dir,
        categories=args.categories,
    )
    
    if args.continuous:
        # Continuous mode
        await fetcher.run_continuous_complete(
            check_interval_hours=args.check_interval,
            max_wait_per_category=args.max_wait_hours,
        )
    else:
        # Process all available dates from past week
        date = None
        if args.date:
            date = datetime.strptime(args.date, "%Y-%m-%d")

        if args.Fetch_Type == 'arXiv':
            await fetcher.run_daily_complete(
                date=date,
                max_wait_hours=args.max_wait_hours,
            )
        elif args.Fetch_Type == 'PubMed':
            await fetcher.async_daily_pubmed(date=date)
        elif args.Fetch_Type == 'all':
            await fetcher.async_daily_pubmed(date=date)
            await fetcher.run_daily_complete(
                date=date,
                max_wait_hours=args.max_wait_hours,
            )


if __name__ == "__main__":
    asyncio.run(main())
