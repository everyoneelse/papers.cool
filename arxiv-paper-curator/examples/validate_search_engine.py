#!/usr/bin/env python3
"""
éªŒè¯æœç´¢å¼•æ“å‡†ç¡®æ€§

æµç¨‹ï¼š
1. ä» arXiv API è·å–æŒ‡å®šæ—¥æœŸ+åˆ†ç±»çš„æ‰€æœ‰è®ºæ–‡
2. ç”¨æœ¬åœ° search_engine æœç´¢å…³é”®è¯ â†’ ç»“æœA
3. ç”¨ arXiv API ç›´æ¥æœç´¢å…³é”®è¯ â†’ ç»“æœB
4. å¯¹æ¯” A å’Œ B
"""

import sys
from pathlib import Path
from datetime import datetime
import json
import logging

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root.parent / "frontend"))

from src.scripts.arxiv_advanced_search import ArxivAdvancedSearch
from search_engine import PaperSearchEngine

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def fetch_all_papers_from_arxiv(date: datetime, categories: list, output_file: Path):
    """
    Step 1: ä» arXiv è·å–æŒ‡å®šæ—¥æœŸå’Œåˆ†ç±»çš„æ‰€æœ‰è®ºæ–‡
    """
    logger.info("="*80)
    logger.info("Step 1: Fetching ALL papers from arXiv API")
    logger.info(f"Date: {date.strftime('%Y-%m-%d')}")
    logger.info(f"Categories: {categories}")
    logger.info("="*80)
    
    searcher = ArxivAdvancedSearch(delay_seconds=3.0)
    
    all_papers = []
    for category in categories:
        logger.info(f"\nFetching category: {category}")
        papers = searcher.search(
            categories=[category],
            date_from=date,
            date_to=date,
            max_results=10000
        )
        
        for paper in papers:
            paper['source_category'] = category
        
        all_papers.extend(papers)
        logger.info(f"  â†’ Found {len(papers)} papers in {category}")
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    output_data = {
        "metadata": {
            "fetch_date": datetime.now().isoformat(),
            "target_date": date.strftime('%Y-%m-%d'),
            "categories": categories,
            "total_papers": len(all_papers),
            "source": "arxiv_api"
        },
        "papers": all_papers
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"\nâœ“ Saved {len(all_papers)} papers to {output_file}")
    return all_papers


def search_with_local_engine(papers: list, keywords: str, phrase_search: bool = False, require_all_words: bool = True, remove_stopwords: bool = False):
    """
    Step 2a: ç”¨æœ¬åœ°æœç´¢å¼•æ“æœç´¢
    """
    logger.info("\n" + "="*80)
    logger.info(f"Step 2a: Searching with LOCAL search engine")
    logger.info(f"Keywords: {keywords}")
    logger.info(f"Phrase search: {phrase_search}")
    logger.info(f"Require all words: {require_all_words}")
    logger.info(f"Remove stopwords: {remove_stopwords}")
    logger.info("="*80)
    
    # åˆ›å»ºæœç´¢å¼•æ“å¹¶æ„å»ºç´¢å¼•
    engine = PaperSearchEngine(index_path="./validation_search_index")
    engine.clear_index()
    engine.build_index_from_papers(papers)
    
    # æœç´¢
    results = engine.search(
        query=keywords, 
        max_results=1000, 
        phrase_search=phrase_search,
        require_all_words=require_all_words,
        remove_stopwords=remove_stopwords
    )
    
    logger.info(f"âœ“ Local search found {len(results)} papers")
    return results


def search_with_arxiv_api(date: datetime, categories: list, keywords: str):
    """
    Step 2b: ç”¨ arXiv API æœç´¢
    """
    logger.info("\n" + "="*80)
    logger.info(f"Step 2b: Searching with arXiv API")
    logger.info(f"Keywords: {keywords}")
    logger.info("="*80)
    
    searcher = ArxivAdvancedSearch(delay_seconds=3.0)
    
    all_results = []
    for category in categories:
        logger.info(f"\nSearching category: {category}")
        papers = searcher.search(
            keywords=keywords,
            categories=[category],
            date_from=date,
            date_to=date,
            max_results=10000
        )
        all_results.extend(papers)
        logger.info(f"  â†’ Found {len(papers)} papers in {category}")
    
    logger.info(f"\nâœ“ arXiv API search found {len(all_results)} papers")
    return all_results


def compare_search_results(local_results: list, arxiv_results: list):
    """
    Step 3: å¯¹æ¯”æœç´¢ç»“æœ
    """
    logger.info("\n" + "="*80)
    logger.info("Step 3: Comparing search results")
    logger.info("="*80)
    
    # æå–IDé›†åˆ
    local_ids = set()
    for paper in local_results:
        paper_id = paper.get('id') or paper.get('arxiv_id', '')
        clean_id = paper_id.split('v')[0] if paper_id else ''
        if clean_id:
            local_ids.add(clean_id)
    
    arxiv_ids = set()
    for paper in arxiv_results:
        paper_id = paper.get('arxiv_id', '')
        clean_id = paper_id.split('v')[0] if paper_id else ''
        if clean_id:
            arxiv_ids.add(clean_id)
    
    # å¯¹æ¯”
    matched = local_ids & arxiv_ids
    missing_in_local = arxiv_ids - local_ids
    extra_in_local = local_ids - arxiv_ids
    
    match_rate = len(matched) / len(arxiv_ids) * 100 if arxiv_ids else 100
    
    # æ‰“å°æŠ¥å‘Š
    print("\n" + "="*80)
    print("SEARCH ENGINE VALIDATION REPORT")
    print("="*80)
    print(f"\nğŸ“Š Statistics:")
    print(f"  Local Search Results:  {len(local_ids)}")
    print(f"  arXiv API Results:     {len(arxiv_ids)}")
    print(f"  Matched:               {len(matched)}")
    print(f"  Match Rate:            {match_rate:.2f}%")
    
    if missing_in_local:
        print(f"\nâš ï¸  Missing in Local Search ({len(missing_in_local)}):")
        for arxiv_id in sorted(list(missing_in_local))[:10]:
            print(f"    - {arxiv_id}")
        if len(missing_in_local) > 10:
            print(f"    ... and {len(missing_in_local) - 10} more")
    
    if extra_in_local:
        print(f"\nâ„¹ï¸  Extra in Local Search ({len(extra_in_local)}):")
        for arxiv_id in sorted(list(extra_in_local))[:10]:
            print(f"    - {arxiv_id}")
        if len(extra_in_local) > 10:
            print(f"    ... and {len(extra_in_local) - 10} more")
    
    if match_rate == 100:
        print(f"\nâœ… PERFECT MATCH! Search engine is 100% accurate.")
    elif match_rate >= 95:
        print(f"\nâœ“ Good match rate. Minor discrepancies detected.")
    else:
        print(f"\nâš ï¸  Match rate is below 95%. Please check search engine logic.")
    
    print("="*80)
    
    return {
        'local_count': len(local_ids),
        'arxiv_count': len(arxiv_ids),
        'matched': len(matched),
        'match_rate': match_rate,
        'missing_in_local': sorted(list(missing_in_local)),
        'extra_in_local': sorted(list(extra_in_local))
    }


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="éªŒè¯æœç´¢å¼•æ“å‡†ç¡®æ€§"
    )
    parser.add_argument(
        '--date',
        type=str,
        required=True,
        help='ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)'
    )
    parser.add_argument(
        '--categories',
        type=str,
        nargs='+',
        default=['cs.AI'],
        help='åˆ†ç±»åˆ—è¡¨'
    )
    parser.add_argument(
        '--keywords',
        type=str,
        required=True,
        help='æœç´¢å…³é”®è¯'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='./validation_output',
        help='è¾“å‡ºç›®å½•'
    )
    parser.add_argument(
        '--skip-fetch',
        action='store_true',
        help='è·³è¿‡è·å–æ­¥éª¤ï¼Œä½¿ç”¨å·²å­˜åœ¨çš„æ•°æ®æ–‡ä»¶'
    )
    parser.add_argument(
        '--phrase-search',
        action='store_true',
        help='ä½¿ç”¨ä¸¥æ ¼çŸ­è¯­æœç´¢ï¼ˆè¿ç»­åŒ¹é…ï¼‰'
    )
    parser.add_argument(
        '--require-all-words',
        action='store_true',
        default=True,
        help='è¦æ±‚æ‰€æœ‰è¯éƒ½å‡ºç°ï¼ˆé»˜è®¤Trueï¼Œä¸arXivä¸€è‡´ï¼‰'
    )
    parser.add_argument(
        '--no-require-all-words',
        action='store_false',
        dest='require_all_words',
        help='ä¸è¦æ±‚æ‰€æœ‰è¯ï¼ˆORæ¨¡å¼ï¼‰'
    )
    parser.add_argument(
        '--remove-stopwords',
        action='store_true',
        help='ç§»é™¤åœç”¨è¯ï¼ˆa, an, the, in, on, ç­‰ï¼‰'
    )
    
    args = parser.parse_args()
    
    # è§£ææ—¥æœŸ
    target_date = datetime.strptime(args.date, '%Y-%m-%d')
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    papers_file = output_dir / f"papers_{args.date}_baseline.json"
    
    # Step 1: è·å–æ‰€æœ‰è®ºæ–‡ï¼ˆæˆ–åŠ è½½å·²æœ‰æ•°æ®ï¼‰
    if args.skip_fetch and papers_file.exists():
        logger.info(f"Loading existing data from {papers_file}")
        with open(papers_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            all_papers = data['papers']
        logger.info(f"âœ“ Loaded {len(all_papers)} papers")
    else:
        all_papers = fetch_all_papers_from_arxiv(
            target_date,
            args.categories,
            papers_file
        )
    
    # Step 2a: æœ¬åœ°æœç´¢å¼•æ“æœç´¢
    local_results = search_with_local_engine(
        all_papers, 
        args.keywords, 
        args.phrase_search,
        args.require_all_words,
        args.remove_stopwords
    )
    
    # Step 2b: arXiv API æœç´¢
    arxiv_results = search_with_arxiv_api(
        target_date,
        args.categories,
        args.keywords
    )
    
    # Step 3: å¯¹æ¯”ç»“æœ
    comparison = compare_search_results(local_results, arxiv_results)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = output_dir / f"validation_report_{args.date}.json"
    report_data = {
        "metadata": {
            "validation_date": datetime.now().isoformat(),
            "target_date": args.date,
            "categories": args.categories,
            "keywords": args.keywords,
        },
        "comparison": comparison,
        "local_results": [
            {
                'id': r.get('id') or r.get('arxiv_id', ''),
                'title': r.get('title', ''),
                'search_score': r.get('search_score', 0)
            }
            for r in local_results
        ],
        "arxiv_results": [
            {
                'arxiv_id': r.get('arxiv_id', ''),
                'title': r.get('title', '')
            }
            for r in arxiv_results
        ]
    }
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"\nğŸ’¾ Validation report saved to {report_file}")


if __name__ == "__main__":
    main()

