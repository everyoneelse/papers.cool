"""
é›†æˆç¤ºä¾‹ï¼šå°† arXiv Advanced Search é›†æˆåˆ°ä½ çš„æœç´¢å¼•æ“ä¸­

å±•ç¤ºå¦‚ä½•åœ¨ç°æœ‰çš„ search_engine.py åŸºç¡€ä¸Šæ·»åŠ å®˜æ–¹æœç´¢éªŒè¯åŠŸèƒ½
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root.parent / "frontend"))

from src.scripts.arxiv_advanced_search import ArxivAdvancedSearch
from datetime import datetime
import json


class EnhancedPaperSearch:
    """
    å¢å¼ºçš„è®ºæ–‡æœç´¢å¼•æ“
    
    ç»“åˆæœ¬åœ°æœç´¢å’Œ arXiv å®˜æ–¹æœç´¢ï¼Œæä¾›æ•°æ®éªŒè¯åŠŸèƒ½
    """
    
    def __init__(self, local_data_dir: str = "./papers_data"):
        self.local_data_dir = Path(local_data_dir)
        self.arxiv_searcher = ArxivAdvancedSearch()
    
    def load_local_papers(self, date: datetime, categories: list) -> dict:
        """åŠ è½½æœ¬åœ°è®ºæ–‡æ•°æ®"""
        papers_by_category = {}
        date_str = date.strftime('%Y-%m-%d')
        
        for category in categories:
            category_file = self.local_data_dir / category / f"papers_{date_str}_100percent.json"
            
            if category_file.exists():
                with open(category_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    papers_by_category[category] = data.get('papers', [])
            else:
                papers_by_category[category] = []
        
        return papers_by_category
    
    def search_with_validation(
        self,
        keywords: str,
        date: datetime,
        categories: list,
        validate: bool = True
    ):
        """
        æœç´¢è®ºæ–‡å¹¶å¯é€‰åœ°éªŒè¯æ•°æ®å®Œæ•´æ€§
        
        Args:
            keywords: æœç´¢å…³é”®è¯
            date: ç›®æ ‡æ—¥æœŸ
            categories: åˆ†ç±»åˆ—è¡¨
            validate: æ˜¯å¦ä¸ arXiv å®˜æ–¹éªŒè¯
            
        Returns:
            æœç´¢ç»“æœå’ŒéªŒè¯æŠ¥å‘Šï¼ˆå¦‚æœå¯ç”¨ï¼‰
        """
        # 1. ä»æœ¬åœ°æ•°æ®æœç´¢
        print(f"ğŸ” Searching local data for '{keywords}' on {date.strftime('%Y-%m-%d')}...")
        local_papers = self.load_local_papers(date, categories)
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼ˆä½ å¯ä»¥æ›¿æ¢ä¸º BM25 æœç´¢ï¼‰
        local_results = []
        for category, papers in local_papers.items():
            for paper in papers:
                if self._keyword_match(keywords, paper):
                    local_results.append({
                        **paper,
                        'source': 'local',
                        'category': category
                    })
        
        print(f"âœ“ Found {len(local_results)} papers in local data")
        
        # 2. å¯é€‰ï¼šä» arXiv å®˜æ–¹æœç´¢éªŒè¯
        validation_report = None
        if validate:
            print(f"\nğŸŒ Validating with arXiv official search...")
            
            arxiv_results = self.arxiv_searcher.search(
                keywords=keywords,
                categories=categories,
                date_from=date,
                date_to=date,
                max_results=1000
            )
            
            print(f"âœ“ Found {len(arxiv_results)} papers on arXiv")
            
            # æ¯”è¾ƒç»“æœ
            local_ids = set(p.get('arxiv_id', '').split('v')[0] for p in local_results)
            arxiv_ids = set(p['arxiv_id'].split('v')[0] for p in arxiv_results)
            
            matched = local_ids & arxiv_ids
            missing_in_local = arxiv_ids - local_ids
            extra_in_local = local_ids - arxiv_ids
            
            match_rate = len(matched) / len(arxiv_ids) * 100 if arxiv_ids else 100
            
            validation_report = {
                'local_count': len(local_results),
                'arxiv_count': len(arxiv_results),
                'matched': len(matched),
                'match_rate': match_rate,
                'missing_in_local': list(missing_in_local),
                'extra_in_local': list(extra_in_local),
            }
            
            print(f"\nğŸ“Š Validation Results:")
            print(f"  Match rate: {match_rate:.1f}%")
            print(f"  Local: {len(local_results)}, arXiv: {len(arxiv_results)}, Matched: {len(matched)}")
            
            if missing_in_local:
                print(f"  âš ï¸  Missing in local: {len(missing_in_local)} papers")
                for arxiv_id in list(missing_in_local)[:3]:
                    print(f"     - {arxiv_id}")
        
        return {
            'results': local_results,
            'validation': validation_report
        }
    
    def _keyword_match(self, keywords: str, paper: dict) -> bool:
        """ç®€å•çš„å…³é”®è¯åŒ¹é…"""
        keywords_lower = keywords.lower()
        
        # åœ¨æ ‡é¢˜å’Œæ‘˜è¦ä¸­æœç´¢
        title = paper.get('title', '').lower()
        abstract = paper.get('abstract', '').lower()
        
        return keywords_lower in title or keywords_lower in abstract
    
    def verify_date_completeness(
        self,
        date: datetime,
        categories: list = None
    ) -> dict:
        """
        éªŒè¯æŒ‡å®šæ—¥æœŸçš„æ•°æ®å®Œæ•´æ€§
        
        Args:
            date: ç›®æ ‡æ—¥æœŸ
            categories: åˆ†ç±»åˆ—è¡¨ï¼ˆé»˜è®¤ï¼šæ‰€æœ‰ AI åˆ†ç±»ï¼‰
            
        Returns:
            å®Œæ•´æ€§æŠ¥å‘Š
        """
        if categories is None:
            categories = ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.NE", "cs.CC", "stat.ML"]
        
        print(f"\n{'='*80}")
        print(f"ğŸ“‹ Data Completeness Verification for {date.strftime('%Y-%m-%d')}")
        print(f"{'='*80}")
        
        # åŠ è½½æœ¬åœ°æ•°æ®
        local_papers = self.load_local_papers(date, categories)
        
        # ä» arXiv è·å–å®˜æ–¹æ•°æ®
        print(f"\nğŸŒ Fetching from arXiv API...")
        arxiv_results = self.arxiv_searcher.search_by_date_and_category(
            date=date,
            categories=categories
        )
        
        # æŒ‰åˆ†ç±»å¯¹æ¯”
        report = {
            'date': date.strftime('%Y-%m-%d'),
            'categories': {},
            'summary': {
                'total_local': 0,
                'total_arxiv': 0,
                'total_matched': 0,
                'overall_match_rate': 0,
            }
        }
        
        for category in categories:
            local = local_papers.get(category, [])
            arxiv = arxiv_results.get(category, [])
            
            local_ids = set(p.get('arxiv_id', '').split('v')[0] for p in local if p.get('arxiv_id'))
            arxiv_ids = set(p['arxiv_id'].split('v')[0] for p in arxiv)
            
            matched = local_ids & arxiv_ids
            missing = arxiv_ids - local_ids
            extra = local_ids - arxiv_ids
            
            match_rate = len(matched) / len(arxiv_ids) * 100 if arxiv_ids else 100
            
            report['categories'][category] = {
                'local_count': len(local),
                'arxiv_count': len(arxiv),
                'matched': len(matched),
                'match_rate': match_rate,
                'missing_count': len(missing),
                'extra_count': len(extra),
                'status': 'âœ…' if match_rate == 100 else ('âœ“' if match_rate >= 95 else 'âš ï¸')
            }
            
            report['summary']['total_local'] += len(local)
            report['summary']['total_arxiv'] += len(arxiv)
            report['summary']['total_matched'] += len(matched)
        
        # è®¡ç®—æ€»ä½“åŒ¹é…ç‡
        if report['summary']['total_arxiv'] > 0:
            report['summary']['overall_match_rate'] = (
                report['summary']['total_matched'] / report['summary']['total_arxiv'] * 100
            )
        
        # æ‰“å°æŠ¥å‘Š
        print(f"\n{'='*80}")
        print(f"ğŸ“Š Summary:")
        print(f"  Total Local:  {report['summary']['total_local']}")
        print(f"  Total arXiv:  {report['summary']['total_arxiv']}")
        print(f"  Match Rate:   {report['summary']['overall_match_rate']:.2f}%")
        print(f"\nğŸ“‹ By Category:")
        
        for cat, cat_report in report['categories'].items():
            print(f"  {cat_report['status']} {cat}: "
                  f"{cat_report['local_count']} local, {cat_report['arxiv_count']} arXiv, "
                  f"{cat_report['match_rate']:.1f}% match")
        
        print(f"{'='*80}\n")
        
        return report


def demo_basic_search():
    """æ¼”ç¤ºåŸºæœ¬æœç´¢åŠŸèƒ½"""
    print("\n" + "="*80)
    print("ğŸ“ Demo 1: Basic Search with Validation")
    print("="*80)
    
    engine = EnhancedPaperSearch()
    
    # æœç´¢ç¤ºä¾‹ï¼ˆä½¿ç”¨ä¸€ä¸ªä½ æœ‰æ•°æ®çš„æ—¥æœŸï¼‰
    # æ³¨æ„ï¼šè¯·æ›¿æ¢ä¸ºä½ å®é™…æœ‰æ•°æ®çš„æ—¥æœŸ
    target_date = datetime(2024, 11, 25)
    
    result = engine.search_with_validation(
        keywords="large language model",
        date=target_date,
        categories=["cs.AI", "cs.CL"],
        validate=True  # å¯ç”¨éªŒè¯
    )
    
    print(f"\nâœ… Search completed")
    print(f"   Found {len(result['results'])} papers")
    
    if result['validation']:
        print(f"   Validation: {result['validation']['match_rate']:.1f}% match rate")


def demo_completeness_check():
    """æ¼”ç¤ºå®Œæ•´æ€§æ£€æŸ¥"""
    print("\n" + "="*80)
    print("ğŸ“ Demo 2: Data Completeness Check")
    print("="*80)
    
    engine = EnhancedPaperSearch()
    
    # æ£€æŸ¥æŒ‡å®šæ—¥æœŸçš„æ•°æ®å®Œæ•´æ€§
    target_date = datetime(2024, 11, 25)
    
    report = engine.verify_date_completeness(
        date=target_date,
        categories=["cs.AI", "cs.CV"]  # åªæ£€æŸ¥éƒ¨åˆ†åˆ†ç±»ä»¥èŠ‚çœæ—¶é—´
    )
    
    # ä¿å­˜æŠ¥å‘Š
    output_dir = Path(__file__).parent / "validation_output"
    output_dir.mkdir(exist_ok=True)
    
    output_file = output_dir / f"completeness_{target_date.strftime('%Y-%m-%d')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ Report saved to: {output_file}")


def demo_custom_integration():
    """æ¼”ç¤ºå¦‚ä½•é›†æˆåˆ°ä½ çš„ç°æœ‰ä»£ç """
    print("\n" + "="*80)
    print("ğŸ“ Demo 3: Integration with Existing Search Engine")
    print("="*80)
    
    # å‡è®¾ä½ å·²ç»æœ‰ search_engine.py çš„ PaperSearchEngine
    # è¿™é‡Œå±•ç¤ºå¦‚ä½•æ·»åŠ éªŒè¯åŠŸèƒ½
    
    print("""
ä½ å¯ä»¥è¿™æ ·é›†æˆåˆ°ç°æœ‰çš„ search_engine.pyï¼š

```python
from search_engine import PaperSearchEngine
from src.scripts.arxiv_advanced_search import ArxivAdvancedSearch

class ValidatedSearchEngine(PaperSearchEngine):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.arxiv_searcher = ArxivAdvancedSearch()
    
    def search_with_validation(self, query, date, categories):
        # 1. ä½¿ç”¨æœ¬åœ° BM25 æœç´¢
        local_results = self.search(query, max_results=100)
        
        # 2. ä» arXiv éªŒè¯
        arxiv_results = self.arxiv_searcher.search(
            keywords=query,
            categories=categories,
            date_from=date,
            date_to=date
        )
        
        # 3. å¯¹æ¯”ç»“æœ
        # ... (å‚è€ƒ EnhancedPaperSearch çš„å®ç°)
        
        return local_results, arxiv_results
```

ç„¶ååœ¨ä½ çš„ Streamlit åº”ç”¨ä¸­ï¼š

```python
# åœ¨ frontend ä»£ç ä¸­
engine = ValidatedSearchEngine()

if st.button("Validate Data Completeness"):
    report = engine.verify_completeness(selected_date, selected_categories)
    st.json(report)
```
    """)


if __name__ == "__main__":
    print("\n" + "="*80)
    print("ğŸš€ Enhanced Paper Search - Integration Examples")
    print("="*80)
    print("\nè¿™ä¸ªè„šæœ¬å±•ç¤ºå¦‚ä½•å°† arXiv Advanced Search é›†æˆåˆ°ä½ çš„æœç´¢å¼•æ“ä¸­")
    print("\nâš ï¸  æ³¨æ„ï¼šä»¥ä¸‹ç¤ºä¾‹éœ€è¦ä½ æœ‰å¯¹åº”æ—¥æœŸçš„æœ¬åœ°æ•°æ®")
    print("   å¦‚æœæ²¡æœ‰ï¼Œè¯·å…ˆè¿è¡Œ: python src/scripts/fetch_daily_papers_100percent.py --date YYYY-MM-DD")
    print("="*80)
    
    # è¿è¡Œæ¼”ç¤º
    # æ³¨æ„ï¼šè¿™äº›æ¼”ç¤ºéœ€è¦å®é™…çš„æ•°æ®ï¼Œä½ å¯ä»¥æ ¹æ®éœ€è¦æ³¨é‡Š/å–æ¶ˆæ³¨é‡Š
    
    # demo_basic_search()  # éœ€è¦æœ¬åœ°æ•°æ®
    # demo_completeness_check()  # éœ€è¦æœ¬åœ°æ•°æ®å’Œç½‘ç»œè¿æ¥
    demo_custom_integration()  # åªå±•ç¤ºä»£ç ï¼Œä¸éœ€è¦æ•°æ®
    
    print("\n" + "="*80)
    print("âœ… Integration examples completed!")
    print("\nğŸ’¡ Next steps:")
    print("  1. åœ¨ä½ çš„ search_engine.py ä¸­æ·»åŠ  ArxivAdvancedSearch")
    print("  2. åœ¨ Streamlit UI ä¸­æ·»åŠ  'éªŒè¯æ•°æ®å®Œæ•´æ€§' æŒ‰é’®")
    print("  3. å®šæœŸè¿è¡Œå®Œæ•´æ€§æ£€æŸ¥ä»¥ç¡®ä¿æ•°æ®è´¨é‡")
    print("="*80 + "\n")

