"""
ÂØπÊØîÊú¨Âú∞Êï∞ÊçÆ‰∏é arXiv ÂÆòÊñπÊêúÁ¥¢ÁªìÊûúÁöÑÁ§∫‰æãËÑöÊú¨

‰ΩøÁî®ÊñπÊ≥ï:
    python compare_local_with_arxiv.py --date 2024-11-25 --keywords "large language model"
    python compare_local_with_arxiv.py --date 2024-11-25 --categories cs.AI cs.CV
    python compare_local_with_arxiv.py --date 2024-11-25  # ÂØπÊØîÊâÄÊúâÂàÜÁ±ª
"""

import sys
import os
from pathlib import Path

# Ê∑ªÂä†È°πÁõÆÊ†πÁõÆÂΩïÂà∞Ë∑ØÂæÑ
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.scripts.arxiv_advanced_search import (
    ArxivAdvancedSearch,
    compare_with_local_data,
    print_comparison_report
)
from datetime import datetime, timedelta
import json
import argparse
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_local_papers_by_announced_date(data_dir: Path, date: datetime, categories: list) -> dict:
    """
    Êåâannounced dateÂä†ËΩΩÊú¨Âú∞ËÆ∫ÊñáÊï∞ÊçÆ
    """
    local_papers = {}
    date_str = date.strftime('%Y-%m-%d')
    
    for category in categories:
        category_file = data_dir / category / f"papers_{date_str}_100percent.json"
        
        if category_file.exists():
            try:
                with open(category_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    papers = data.get('papers', [])
                    local_papers[category] = papers
                    logger.info(f"‚úì Loaded {len(papers)} papers from {category}")
            except Exception as e:
                logger.error(f"‚úó Error loading {category_file}: {e}")
                local_papers[category] = []
        else:
            logger.warning(f"‚ö† File not found: {category_file}")
            local_papers[category] = []
    
    return local_papers


def reorganize_by_submitted_date(papers_by_category: dict, target_submitted_date: datetime) -> dict:
    """
    Â∞ÜÊåâannounced dateÁªÑÁªáÁöÑËÆ∫ÊñáÈáçÊñ∞Êåâsubmitted dateÁªÑÁªá
    
    Args:
        papers_by_category: ÊåâcategoryÁªÑÁªáÁöÑËÆ∫Êñá
        target_submitted_date: ÁõÆÊ†ásubmitted date
        
    Returns:
        Êåâsubmitted dateËøáÊª§ÂêéÁöÑËÆ∫ÊñáÂ≠óÂÖ∏
    """
    from collections import defaultdict
    
    target_date_str = target_submitted_date.strftime('%Y-%m-%d')
    reorganized = {}
    
    for category, papers in papers_by_category.items():
        reorganized[category] = []
        for paper in papers:
            paper_submitted_date_raw = paper.get('published_date', '')
            try:
                # Â∞ÜISOÊ†ºÂºèÊó•ÊúüËΩ¨Êç¢‰∏∫YYYY-MM-DDÊ†ºÂºè
                if paper_submitted_date_raw:
                    paper_submitted_date = datetime.fromisoformat(paper_submitted_date_raw.replace('Z', '+00:00')).strftime('%Y-%m-%d')
                else:
                    paper_submitted_date = ''
            except (ValueError, TypeError):
                # Â¶ÇÊûúËß£ÊûêÂ§±Ë¥•Ôºå‰ΩøÁî®ÂéüÂßãÂÄºÊàñÁ©∫Â≠óÁ¨¶‰∏≤
                paper_submitted_date = paper_submitted_date_raw if paper_submitted_date_raw else ''
            #import pdb; pdb.set_trace()
            if paper_submitted_date == target_date_str:
                reorganized[category].append(paper)
        
        logger.info(f"  {category}: {len(reorganized[category])} papers with submitted date {target_date_str}")
    
    return reorganized


def load_local_papers_around_date(data_dir: Path, target_date: datetime, categories: list, days_range: int = 7) -> dict:
    """
    Âä†ËΩΩÁõÆÊ†áÊó•ÊúüÂâçÂêéÂá†Â§©ÁöÑannounced dateÊï∞ÊçÆÔºåÁî®‰∫éÂêéÁª≠Êåâsubmitted dateÈáçÁªÑ
    
    Args:
        data_dir: Êï∞ÊçÆÁõÆÂΩï
        target_date: ÁõÆÊ†áÊó•Êúü
        categories: ÂàÜÁ±ªÂàóË°®
        days_range: ÂêëÂâçÂêëÂêéÂä†ËΩΩÁöÑÂ§©Êï∞ËåÉÂõ¥
        
    Returns:
        ÂêàÂπ∂ÂêéÁöÑËÆ∫ÊñáÂ≠óÂÖ∏ÔºàÊåâcategoryÁªÑÁªáÔºâ
    """
    from datetime import timedelta
    from collections import defaultdict
    
    merged_papers = defaultdict(list)
    seen_ids = defaultdict(set)
    
    for offset in range(-days_range, days_range + 1):
        check_date = target_date + timedelta(days=offset)
        date_papers = load_local_papers_by_announced_date(data_dir, check_date, categories)
        
        for category, papers in date_papers.items():
            for paper in papers:
                paper_id = paper.get('arxiv_id', '')
                if paper_id and paper_id not in seen_ids[category]:
                    merged_papers[category].append(paper)
                    seen_ids[category].add(paper_id)
    
    total = sum(len(papers) for papers in merged_papers.values())
    logger.info(f"‚úì Loaded total {total} unique papers from ¬±{days_range} days around {target_date.strftime('%Y-%m-%d')}")
    
    return dict(merged_papers)


def main():
    parser = argparse.ArgumentParser(
        description="ÂØπÊØîÊú¨Âú∞ËÆ∫ÊñáÊï∞ÊçÆ‰∏é arXiv ÂÆòÊñπÊêúÁ¥¢ÁªìÊûú",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Á§∫‰æã:
  # ÂØπÊØîÊåáÂÆöÊó•ÊúüÁöÑÊâÄÊúâÂàÜÁ±ª
  python compare_local_with_arxiv.py --date 2024-11-25
  
  # ÂØπÊØîÊåáÂÆöÊó•ÊúüÂíåÂàÜÁ±ª
  python compare_local_with_arxiv.py --date 2024-11-25 --categories cs.AI cs.CV
  
  # ‰ΩøÁî®ÂÖ≥ÈîÆËØçËøáÊª§
  python compare_local_with_arxiv.py --date 2024-11-25 --keywords "large language model"
  
  # ‰øùÂ≠òÁªìÊûú
  python compare_local_with_arxiv.py --date 2024-11-25 --output ./comparison_results
        """
    )
    
    parser.add_argument(
        '--date',
        type=str,
        required=True,
        help='ÁõÆÊ†áÊó•Êúü (YYYY-MM-DD)'
    )
    parser.add_argument(
        '--categories',
        type=str,
        nargs='+',
        default=['cs.AI', 'cs.CL', 'cs.CV', 'cs.LG', 'cs.NE', 'cs.CC', 'stat.ML'],
        help='Ë¶ÅÂØπÊØîÁöÑÂàÜÁ±ªÂàóË°® (ÈªòËÆ§: ÊâÄÊúâ AI Áõ∏ÂÖ≥ÂàÜÁ±ª)'
    )
    parser.add_argument(
        '--keywords',
        type=str,
        help='ÂèØÈÄâÁöÑÂÖ≥ÈîÆËØçËøáÊª§'
    )
    parser.add_argument(
        '--local-data-dir',
        type=str,
        default='./papers_data',
        help='Êú¨Âú∞Êï∞ÊçÆÁõÆÂΩï (ÈªòËÆ§: ./papers_data)'
    )
    parser.add_argument(
        '--output',
        type=str,
        help='ËæìÂá∫ÁõÆÂΩïÔºà‰øùÂ≠òÂØπÊØîÁªìÊûúÂíåÊä•ÂëäÔºâ'
    )
    parser.add_argument(
        '--delay',
        type=float,
        default=3.0,
        help='API ËØ∑Ê±ÇÈó¥ÈöîÁßíÊï∞ (ÈªòËÆ§: 3.0)'
    )
    parser.add_argument(
        '--by-submitted-date',
        action='store_true',
        help='Êåâ submitted date ÂØπÊØîÔºàÈªòËÆ§Êåâ announced dateÔºâ'
    )
    
    args = parser.parse_args()
    
    # Ëß£ÊûêÊó•Êúü
    try:
        target_date = datetime.strptime(args.date, '%Y-%m-%d')
    except ValueError:
        logger.error(f"Invalid date format: {args.date}. Use YYYY-MM-DD")
        return
    
    logger.info("=" * 80)
    logger.info("üîç arXiv Data Comparison Tool")
    logger.info("=" * 80)
    logger.info(f"üìÖ Date: {target_date.strftime('%Y-%m-%d')}")
    logger.info(f"üìÖ Date Type: {'submitted date' if args.by_submitted_date else 'announced date'}")
    logger.info(f"üìÇ Categories: {', '.join(args.categories)}")
    if args.keywords:
        logger.info(f"üîë Keywords: {args.keywords}")
    logger.info(f"üìÅ Local data dir: {args.local_data_dir}")
    logger.info("=" * 80)
    
    local_data_dir = Path(args.local_data_dir)
    if not local_data_dir.exists():
        logger.error(f"Local data directory not found: {local_data_dir}")
        return
    
    if args.by_submitted_date:
        # Êåâsubmitted dateÂØπÊØîÊ®°Âºè
        logger.info("\nüíæ Step 1: Loading local data around target date...")
        all_local_papers = load_local_papers_around_date(
            local_data_dir, target_date, args.categories, days_range=7
        )
        
        logger.info(f"\nüîÑ Step 2: Reorganizing by submitted date {target_date.strftime('%Y-%m-%d')}...")
        local_papers = reorganize_by_submitted_date(all_local_papers, target_date)
        total_local = sum(len(papers) for papers in local_papers.values())
        logger.info(f"‚úì Found {total_local} papers with submitted date {target_date.strftime('%Y-%m-%d')}")
        
        logger.info("\nüåê Step 3: Fetching from arXiv API (by submitted date)...")
        searcher = ArxivAdvancedSearch(delay_seconds=args.delay)
        arxiv_results = searcher.search_by_date_and_category(
            date=target_date,
            categories=args.categories,
            keywords=args.keywords,
        )
        total_arxiv = sum(len(papers) for papers in arxiv_results.values())
        logger.info(f"‚úì Retrieved {total_arxiv} papers from arXiv API")
    else:
        # Êåâannounced dateÂØπÊØîÊ®°ÂºèÔºàÂéüÊúâÈÄªËæëÔºâ
        logger.info("\nüíæ Step 1: Loading local data...")
        local_papers = load_local_papers_by_announced_date(local_data_dir, target_date, args.categories)
        total_local = sum(len(papers) for papers in local_papers.values())
        logger.info(f"‚úì Loaded {total_local} papers from local storage")
        
        logger.info("\nüåê Step 2: Fetching from arXiv API...")
        searcher = ArxivAdvancedSearch(delay_seconds=args.delay)
        arxiv_results = searcher.search_by_date_and_category(
            date=target_date,
            categories=args.categories,
            keywords=args.keywords,
        )
        total_arxiv = sum(len(papers) for papers in arxiv_results.values())
        logger.info(f"‚úì Retrieved {total_arxiv} papers from arXiv API")
    
    # 3Ô∏è‚É£ ÊâßË°åÂØπÊØî
    logger.info("\n‚öñÔ∏è  Step 3: Comparing results...")
    report = compare_with_local_data(arxiv_results, local_papers, target_date)
    
    # 4Ô∏è‚É£ ÊâìÂç∞Êä•Âëä
    print_comparison_report(report)
    
    # 5Ô∏è‚É£ ‰øùÂ≠òÁªìÊûúÔºàÂèØÈÄâÔºâ
    if args.output:
        output_dir = Path(args.output)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ‰øùÂ≠ò arXiv ÊêúÁ¥¢ÁªìÊûú
        arxiv_file = output_dir / f"arxiv_results_{target_date.strftime('%Y-%m-%d')}.json"
        with open(arxiv_file, 'w', encoding='utf-8') as f:
            json.dump(arxiv_results, f, ensure_ascii=False, indent=2)
        logger.info(f"\nüíæ Saved arXiv results to: {arxiv_file}")
        
        # ‰øùÂ≠òÂØπÊØîÊä•Âëä
        report_file = output_dir / f"comparison_report_{target_date.strftime('%Y-%m-%d')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        logger.info(f"üíæ Saved comparison report to: {report_file}")
        
        # ÁîüÊàêËØ¶ÁªÜÁöÑ Markdown Êä•Âëä
        md_file = output_dir / f"comparison_report_{target_date.strftime('%Y-%m-%d')}.md"
        generate_markdown_report(report, arxiv_results, local_papers, md_file)
        logger.info(f"üíæ Saved Markdown report to: {md_file}")
    
    # 6Ô∏è‚É£ ÊòæÁ§∫Âª∫ËÆÆ
    print("\n" + "=" * 80)
    print("üí° Recommendations:")
    
    if report['summary']['total_missing_in_local'] > 0:
        print(f"  ‚ö†Ô∏è  You have {report['summary']['total_missing_in_local']} papers missing in local storage.")
        print(f"     Consider re-running the fetch script for {target_date.strftime('%Y-%m-%d')}")
    
    if report['summary']['overall_match_rate'] == 100:
        print(f"  ‚úÖ Perfect match! Your local data is 100% complete.")
    elif report['summary']['overall_match_rate'] >= 95:
        print(f"  ‚úì Good match rate ({report['summary']['overall_match_rate']:.1f}%). Minor discrepancies detected.")
    else:
        print(f"  ‚ö†Ô∏è  Match rate is {report['summary']['overall_match_rate']:.1f}%. Please check your fetch process.")
    
    print("=" * 80)


def generate_markdown_report(report: dict, arxiv_results: dict, local_papers: dict, output_file: Path):
    """ÁîüÊàêËØ¶ÁªÜÁöÑ Markdown Ê†ºÂºèÊä•Âëä"""
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"# arXiv Data Comparison Report\n\n")
        f.write(f"**Date:** {report['date']}\n\n")
        f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # ÊÄª‰ΩìÁªüËÆ°
        f.write("## üìä Overall Statistics\n\n")
        summary = report['summary']
        f.write(f"| Metric | Count |\n")
        f.write(f"|--------|-------|\n")
        f.write(f"| arXiv Official | {summary['total_arxiv']} |\n")
        f.write(f"| Local Data | {summary['total_local']} |\n")
        f.write(f"| Matched | {summary['total_matched']} |\n")
        f.write(f"| Match Rate | {summary['overall_match_rate']:.2f}% |\n")
        f.write(f"| Missing in Local | {summary['total_missing_in_local']} |\n")
        f.write(f"| Extra in Local | {summary['total_extra_in_local']} |\n\n")
        
        # ÂàÜÁ±ªËØ¶ÊÉÖ
        f.write("## üìã Category Details\n\n")
        
        for category in sorted(report['categories'].keys()):
            cat_report = report['categories'][category]
            
            # Áä∂ÊÄÅÂõæÊ†á
            if cat_report['match_rate'] == 100:
                status = "‚úÖ"
            elif cat_report['match_rate'] >= 95:
                status = "‚úì"
            else:
                status = "‚ö†Ô∏è"
            
            f.write(f"### {status} {category}\n\n")
            f.write(f"- **arXiv:** {cat_report['arxiv_count']} papers\n")
            f.write(f"- **Local:** {cat_report['local_count']} papers\n")
            f.write(f"- **Matched:** {cat_report['matched_count']} ({cat_report['match_rate']:.1f}%)\n")
            
            # Áº∫Â§±ÁöÑËÆ∫Êñá
            if cat_report['missing_in_local_count'] > 0:
                f.write(f"\n**‚ö†Ô∏è Missing in Local ({cat_report['missing_in_local_count']}):**\n\n")
                for arxiv_id in cat_report['missing_ids']:
                    f.write(f"- [{arxiv_id}](https://arxiv.org/abs/{arxiv_id})\n")
            
            # È¢ùÂ§ñÁöÑËÆ∫Êñá
            if cat_report['extra_in_local_count'] > 0:
                f.write(f"\n**‚ÑπÔ∏è Extra in Local ({cat_report['extra_in_local_count']}):**\n\n")
                for arxiv_id in cat_report['extra_ids']:
                    f.write(f"- [{arxiv_id}](https://arxiv.org/abs/{arxiv_id})\n")
            
            f.write("\n")
        
        # Âª∫ËÆÆ
        f.write("## üí° Recommendations\n\n")
        if summary['overall_match_rate'] == 100:
            f.write("‚úÖ **Perfect match!** Your local data is 100% complete.\n")
        elif summary['overall_match_rate'] >= 95:
            f.write(f"‚úì **Good match rate** ({summary['overall_match_rate']:.1f}%). Minor discrepancies detected.\n")
        else:
            f.write(f"‚ö†Ô∏è **Match rate is {summary['overall_match_rate']:.1f}%**. Please check your fetch process.\n")
        
        if summary['total_missing_in_local'] > 0:
            f.write(f"\n‚ö†Ô∏è You have **{summary['total_missing_in_local']} papers missing** in local storage. ")
            f.write(f"Consider re-running the fetch script for {report['date']}.\n")


if __name__ == "__main__":
    main()

